{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yspennstate/RMT_pruning_2/blob/main/rmt_pruning_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uRm9_XKImzk",
        "outputId": "0492ccb8-4035-4c1a-814e-f309d109377b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tracywidom\n",
            "  Downloading TracyWidom-0.3.0.tar.gz (12 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from tracywidom) (1.25.2)\n",
            "Requirement already satisfied: scipy>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tracywidom) (1.11.4)\n",
            "Building wheels for collected packages: tracywidom\n",
            "  Building wheel for tracywidom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tracywidom: filename=TracyWidom-0.3.0-py3-none-any.whl size=11726 sha256=7b5c759477451adfdf05f9b807f3254206d5f20f6a546c52a557643845bf80e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/d7/24/27c1c32a4f1030307028bfedb8bb7de2968c4ea64103705110\n",
            "Successfully built tracywidom\n",
            "Installing collected packages: tracywidom\n",
            "Successfully installed tracywidom-0.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install tracywidom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSUjIIWd76lT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import copy\n",
        "import math\n",
        "import scipy\n",
        "from scipy.integrate import quad\n",
        "from TracyWidom import TracyWidom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1ao82vDIwrd"
      },
      "source": [
        "The next block contains all the code necessary to create the neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qf0mwKyf78qH"
      },
      "outputs": [],
      "source": [
        "class networkModel(nn.Module):\n",
        "  def __init__(self, without_rel, dims):\n",
        "    super(networkModel, self).__init__()\n",
        "    self.without_rel = without_rel\n",
        "    self.fc = nn.ModuleList()\n",
        "    self.dims = dims\n",
        "    current_dim = dims[0]\n",
        "    for i in range(1, len(dims)):\n",
        "      X = nn.Linear(current_dim, dims[i], bias=True)\n",
        "      self.fc.append(X)\n",
        "      current_dim = dims[i]\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.flatten(x,start_dim=2)\n",
        "    for j in range(len(self.fc)):\n",
        "      x = self.fc[j](x)\n",
        "      if not self.without_rel[j]:\n",
        "        x = F.relu(x)\n",
        "    x = torch.flatten(x, start_dim = 1)\n",
        "    x = F.log_softmax(x, dim=1)\n",
        "    return (x)\n",
        "\n",
        "  def getLayers(self):\n",
        "    return len(self.fc)\n",
        "\n",
        "  def getLayerMatrix(self,index):\n",
        "    layerMatrix = torch.as_tensor(self.fc[index].weight)\n",
        "    layerMatrix = layerMatrix.cpu()\n",
        "    layerMatrix = layerMatrix.detach().numpy()\n",
        "    return layerMatrix\n",
        "\n",
        "  def getWithout(self):\n",
        "    return self.without_rel\n",
        "\n",
        "  def getDims(self):\n",
        "    return self.dims\n",
        "\n",
        "  def getParameterCount(self):\n",
        "    count = 0\n",
        "    for param in self.parameters():\n",
        "      temp = 1\n",
        "      param = param.size()\n",
        "      for i in param: temp *= i\n",
        "      count += temp\n",
        "    return count\n",
        "\n",
        "\n",
        "\n",
        "class NetworkModelModified(nn.Module):\n",
        "  def __init__(self, dims, relu_layers, alpha, beta, goodnessOfFitCutoff):\n",
        "        super(NetworkModelModified, self).__init__()\n",
        "        layers = []\n",
        "        for i in range(len(dims) - 1):\n",
        "            layers.append(SplittableLinear(dims[i], dims[i + 1], alpha, beta, goodnessOfFitCutoff, name=f'layer {i+1}'))\n",
        "            if relu_layers[i]:\n",
        "                layers.append(nn.ReLU())\n",
        "        self.model = nn.Sequential(*layers)\n",
        "        self._initialize_weights()\n",
        "\n",
        "  def forward(self, x):\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        return self.model(x)\n",
        "\n",
        "  def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, mean=0, std=1/np.sqrt(m.weight.size(1)))\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "\n",
        "  def getLayerMatrix(self,index):\n",
        "    layerMatrix = torch.as_tensor(self.fc[index].weight)\n",
        "    layerMatrix = layerMatrix.cpu()\n",
        "    layerMatrix = layerMatrix.detach().numpy()\n",
        "    return layerMatrix\n",
        "\n",
        "  def getWithout(self):\n",
        "    return self.without_rel\n",
        "\n",
        "  def getDims(self):\n",
        "    return self.dims\n",
        "\n",
        "  def getParameterCount(self):\n",
        "    count = 0\n",
        "    for param in self.parameters():\n",
        "      temp = 1\n",
        "      param = param.size()\n",
        "      for i in param: temp *= i\n",
        "      count += temp\n",
        "    return count\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch, showTrainingLoss=True):\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    l1_lambda = 0.000005  # Adjust this value based on your requirement\n",
        "    l2_lambda = 0.000005  # Adjust this value based on your requirement\n",
        "\n",
        "    correct = 0\n",
        "    total_loss = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        # L1 regularization\n",
        "        l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "        loss += l1_lambda * l1_norm\n",
        "\n",
        "        # L2 regularization\n",
        "        l2_norm = sum((p ** 2).sum() for p in model.parameters())\n",
        "        loss += l2_lambda * l2_norm\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (batch_idx + 1) % args['log_interval'] == 0 and showTrainingLoss:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    accuracy = 100. * correct / len(train_loader.dataset)\n",
        "\n",
        "    print(f'Train set: Average loss: {avg_loss:.4f}, Accuracy: {correct}/{len(train_loader.dataset)} ({accuracy:.0f}%)')\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def test(args, m, device, test_loader):\n",
        "  m.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      (output) = m(data)\n",
        "      test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "      pred = output.argmax(dim=1, keepdim=True)\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "  test_loss /= len(test_loader)\n",
        "  print(f'test set: Average loss: {test_loss:4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100.*correct/len(test_loader.dataset):.0f}%)')\n",
        "  return (100 * correct / len(test_loader.dataset))\n",
        "\n",
        "\n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "def neuralInit(seed, device, model, lr, momentum, batchSize):\n",
        "    args = {\"batch_size\": batchSize, \"test_batch_size\": batchSize,  \"lr\": lr, \"momentum\": momentum,  \"no_cuda\": False,  \"seed\": seed,  \"log_interval\":50}\n",
        "    use_cuda = not args['no_cuda'] and torch.cuda.is_available()\n",
        "    torch.manual_seed(seed)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    model.to(device)\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "\n",
        "    # Load training data\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        datasets.FashionMNIST('../data', train=True, download=True, transform=transform),\n",
        "        batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
        "\n",
        "    # Load test data\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        datasets.FashionMNIST('../data', train=False, transform=transform),\n",
        "        batch_size=args['test_batch_size'], shuffle=True, **kwargs)\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "    return args, model, optimizer, test_loader, train_loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "br7JclbOpPFi"
      },
      "outputs": [],
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self,  conv_weights, fc_weights, without_rel):\n",
        "      super(CNNModel, self).__init__()\n",
        "\n",
        "      self.dims_conv = conv_weights\n",
        "      self.dims_fc = fc_weights\n",
        "\n",
        "      self.without_rel = without_rel\n",
        "\n",
        "      self.fc = nn.ModuleList()\n",
        "      self.conv = nn.ModuleList()\n",
        "      self.bn_conv = nn.ModuleList()\n",
        "      self.bn_fc = nn.ModuleList()\n",
        "\n",
        "\n",
        "      for k in range(1,len(conv_weights)):\n",
        "        self.conv.append(nn.Conv2d(conv_weights[k-1], conv_weights[k], 3))\n",
        "        self.bn_conv.append(nn.BatchNorm2d(conv_weights[k]))\n",
        "\n",
        "      for k in range(1,len(fc_weights)):\n",
        "        self.fc.append(nn.Linear(fc_weights[k-1],fc_weights[k]))\n",
        "        self.bn_fc.append(nn.BatchNorm1d(fc_weights[k]))\n",
        "\n",
        "      self.bn = [self.bn_conv, self.bn_fc]\n",
        "\n",
        "    def forward(self,x):\n",
        "      for k in range(len(self.conv)):\n",
        "        x = self.conv[k](x)\n",
        "        if self.without_rel[0][k]:\n",
        "          x = F.relu(x)\n",
        "        x = self.bn[0][k](x)\n",
        "        if ((k%2 == 0) and (k != 0)):\n",
        "          x = F.max_pool2d(x, 2)\n",
        "        x = F.dropout(x, 0.35)\n",
        "\n",
        "      x = torch.flatten(x, start_dim = 1)\n",
        "      x = x.reshape([x.shape[0],x.shape[1]])\n",
        "\n",
        "      for k in range(len(self.fc)):\n",
        "        x = self.fc[k](x)\n",
        "        if k == 0:\n",
        "          x = self.bn[1][k](x)\n",
        "          if self.without_rel[1][k]:\n",
        "            x = F.relu(x)\n",
        "          x = F.dropout(x)\n",
        "      #x = DropConnect(x, 0.5)\n",
        "      x = F.log_softmax(x, dim=0)\n",
        "      return(x)\n",
        "\n",
        "\n",
        "    def getLayers(self):\n",
        "      return len(self.conv)\n",
        "\n",
        "    def getLayerMatrixOriginal(self,index):\n",
        "      layerMatrix = torch.as_tensor(self.conv[index].weight)\n",
        "      layerMatrix = layerMatrix.cpu()\n",
        "      layerMatrix = layerMatrix.detach().numpy()\n",
        "      return layerMatrix\n",
        "\n",
        "    def getLayerMatrixConv(self,index):\n",
        "      m = self.conv[index].weight.shape\n",
        "      layerMatrix = torch.as_tensor(self.conv[index].weight.reshape(m[0],m[1]*m[2]*m[3]))\n",
        "      layerMatrix = layerMatrix.cpu()\n",
        "      layerMatrix = layerMatrix.detach().numpy()\n",
        "      return layerMatrix\n",
        "\n",
        "    def getLayerMatrixFC(self,index):\n",
        "      layerMatrix = torch.as_tensor(self.fc[index].weight)\n",
        "      layerMatrix = layerMatrix.cpu()\n",
        "      layerMatrix = layerMatrix.detach().numpy()\n",
        "      return layerMatrix\n",
        "\n",
        "    def getWithout(self):\n",
        "      return self.without_rel\n",
        "\n",
        "    def getDims(self):\n",
        "      return [self.dims_conv, self.dims_fc]\n",
        "\n",
        "    def getDimsFC(self):\n",
        "      return self.dims_fc\n",
        "\n",
        "    def getDimsConv(self):\n",
        "      return self.dims_conv\n",
        "\n",
        "    def getParameterCount(self):\n",
        "      count = 0\n",
        "      for param in self.parameters():\n",
        "        temp = 1\n",
        "        param = param.size()\n",
        "        for i in param: temp *= i\n",
        "        count += temp\n",
        "      return count\n",
        "\n",
        "class CNNModelModified(nn.Module):\n",
        "    def __init__(self,  conv_weights, fc_weights, without_rel,alpha, beta, goodnessOfFitCutoff):\n",
        "      super(CNNModelModified, self).__init__()\n",
        "\n",
        "      self.dims_conv = conv_weights\n",
        "      self.dims_fc = fc_weights\n",
        "\n",
        "      self.without_rel = without_rel\n",
        "\n",
        "      self.fc = nn.ModuleList()\n",
        "      self.conv = nn.ModuleList()\n",
        "      self.bn_conv = nn.ModuleList()\n",
        "      self.bn_fc = nn.ModuleList()\n",
        "\n",
        "\n",
        "      for k in range(1,len(conv_weights)):\n",
        "        X = SplittableConv(\n",
        "            conv_weights[k-1],\n",
        "            conv_weights[k],\n",
        "            3,\n",
        "            alpha=alpha,\n",
        "            beta=beta,\n",
        "            goodnessOfFitCutoff=goodnessOfFitCutoff[0],\n",
        "            name=f'conv layer {k}'\n",
        "            )\n",
        "\n",
        "        self.conv.append(X)\n",
        "        self.bn_conv.append(nn.BatchNorm2d(conv_weights[k]))\n",
        "\n",
        "      for k in range(1,len(fc_weights)):\n",
        "        X = SplittableLinear(\n",
        "            fc_weights[k-1],\n",
        "            fc_weights[k],\n",
        "            alpha=alpha,\n",
        "            beta=beta,\n",
        "            goodnessOfFitCutoff=goodnessOfFitCutoff[1],\n",
        "            name=f'fc layer {k}',\n",
        "            )\n",
        "        self.fc.append(X)\n",
        "        self.bn_fc.append(nn.BatchNorm1d(fc_weights[k]))\n",
        "\n",
        "      self.bn = [self.bn_conv, self.bn_fc]\n",
        "\n",
        "    def forward(self,x):\n",
        "      for k in range(len(self.conv)):\n",
        "        x = self.conv[k](x)\n",
        "        if self.without_rel[0][k]:\n",
        "          x = F.relu(x)\n",
        "        x = self.bn[0][k](x)\n",
        "        if ((k%2 == 0) and (k != 0)):\n",
        "          x = F.max_pool2d(x, 2)\n",
        "        x = F.dropout(x, 0.35)\n",
        "\n",
        "      x = torch.flatten(x, start_dim = 1)\n",
        "      x = x.reshape([x.shape[0],x.shape[1]])\n",
        "\n",
        "      for k in range(len(self.fc)):\n",
        "        x = self.fc[k](x)\n",
        "        if k == 0:\n",
        "          x = self.bn[1][k](x)\n",
        "          if self.without_rel[1][k]:\n",
        "            x = F.relu(x)\n",
        "          x = F.dropout(x)\n",
        "      x = F.log_softmax(x, dim=0)\n",
        "      return(x)\n",
        "\n",
        "    def getLayers(self):\n",
        "      return len(self.conv)\n",
        "\n",
        "    def getLayerMatrixOriginal(self,index):\n",
        "      layerMatrix = torch.as_tensor(self.conv[index].weight)\n",
        "      layerMatrix = layerMatrix.cpu()\n",
        "      layerMatrix = layerMatrix.detach().numpy()\n",
        "      return layerMatrix\n",
        "\n",
        "    def getLayerMatrixConv(self,index):\n",
        "      m = self.conv[index].weight.shape\n",
        "      layerMatrix = torch.as_tensor(self.conv[index].weight.reshape(m[0],m[1]*m[2]*m[3]))\n",
        "      layerMatrix = layerMatrix.cpu()\n",
        "      layerMatrix = layerMatrix.detach().numpy()\n",
        "      return layerMatrix\n",
        "\n",
        "    def getLayerMatrixFC(self,index):\n",
        "      layerMatrix = torch.as_tensor(self.fc[index].weight)\n",
        "      layerMatrix = layerMatrix.cpu()\n",
        "      layerMatrix = layerMatrix.detach().numpy()\n",
        "      return layerMatrix\n",
        "\n",
        "    def getWithout(self):\n",
        "      return self.without_rel\n",
        "\n",
        "    def getDims(self):\n",
        "      return [self.dims_conv, self.dims_fc]\n",
        "\n",
        "    def getDimsFC(self):\n",
        "      return self.dims_fc\n",
        "\n",
        "    def getDimsConv(self):\n",
        "      return self.dims_conv\n",
        "\n",
        "    def getParameterCount(self):\n",
        "      count = 0\n",
        "      for param in self.parameters():\n",
        "        temp = 1\n",
        "        param = param.size()\n",
        "        for i in param: temp *= i\n",
        "        count += temp\n",
        "      return count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AcbsQnnI81h"
      },
      "source": [
        "The next block contains all the code necessary to find lambda_+ according to the BEMA algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXkqRySMI-yT"
      },
      "outputs": [],
      "source": [
        "def mpDensity(ndf, pdim, var = 1):\n",
        "  gamma = ndf/pdim\n",
        "  inv_gamma_sqrt = math.sqrt(1/gamma)\n",
        "  a = var*(1-inv_gamma_sqrt)**2\n",
        "  b = var*(1+inv_gamma_sqrt)**2\n",
        "  return a,b\n",
        "\n",
        "def dmp(x, ndf, pdim, var=1, log = False):\n",
        "  gamma = ndf/pdim\n",
        "\n",
        "  a,b = mpDensity(ndf, pdim, var)\n",
        "\n",
        "  if not log :\n",
        "    # we have to handle +/- zero carefully when gamma=1\n",
        "    if gamma == 1 and x == 0 and 1/x > 0:\n",
        "      d = math.inf\n",
        "    elif x <= a and x >= b:\n",
        "      d = 0\n",
        "    else:\n",
        "      d = gamma/( 2*math.pi*var*x ) * math.sqrt( ( x-a )*( b-x ) )\n",
        "  else:\n",
        "    if gamma == 1 and x == 0 and 1/x > 0:\n",
        "      d = math.inf\n",
        "    elif x <= a and x >= b:\n",
        "      d = -math.inf\n",
        "    else:\n",
        "      d = log( gamma ) - ( log( 2 ) + log( math.pi ) + log( var ) + log( x ) ) + 0.5*log( x-a ) + 0.5*log( b-x )\n",
        "\n",
        "  return d\n",
        "\n",
        "def pmp(q, ndf, pdim, var=1, lower_tail = True, log_p = False):\n",
        "  gamma = ndf/pdim\n",
        "  a,b = mpDensity(ndf, pdim, var)\n",
        "  f = lambda x : dmp(x, ndf, pdim, var)\n",
        "  if lower_tail:\n",
        "    if q<=a:\n",
        "      p = 0\n",
        "    elif q>=b:\n",
        "      p = 1\n",
        "    else:\n",
        "      p = quad(f,a,q)[0]\n",
        "    if gamma < 1 and q >= 0:\n",
        "      p+= (1 - gamma)\n",
        "  else:\n",
        "    if q<=a:\n",
        "      p = min(1, gamma)\n",
        "    elif q>=b:\n",
        "      p = 0\n",
        "    else:\n",
        "      p = quad(f,q,b)[0]\n",
        "    if gamma < 1 and q <= 0:\n",
        "      p+= (1 - gamma)\n",
        "  if log_p:\n",
        "    res = math.log(p)\n",
        "  else:\n",
        "    res = p\n",
        "  return res\n",
        "\n",
        "def qmp(p, ndf, pdim, var=1, lower_tail = True, log_p = False ):\n",
        "  svr = ndf/pdim\n",
        "  if lower_tail:\n",
        "    p = p\n",
        "  else:\n",
        "    p = 1- p\n",
        "  if log_p:\n",
        "    p = math.exp(p)\n",
        "  a,b = mpDensity(ndf, pdim, var)\n",
        "  q = None\n",
        "  if p<=0:\n",
        "    if svr <=1:\n",
        "      q = -0\n",
        "    else:\n",
        "      q = a\n",
        "  else:\n",
        "    if p>=1:\n",
        "      q = b\n",
        "  if svr<1:\n",
        "    if p<1-svr:\n",
        "      q = -0\n",
        "    else:\n",
        "      if p == 1-svr:\n",
        "        q = 0\n",
        "  if q is None:\n",
        "    F = lambda x: pmp(x,ndf, pdim, var) - p\n",
        "    q = scipy.optimize.brentq(F, a, b)\n",
        "  return q\n",
        "\n",
        "\n",
        "\n",
        "#bema_inside is where the BEMA algorithm is calculated\n",
        "#use bema_mat_wrapper instead\n",
        "def bema_inside(pdim, ndf, eigs, alpha, beta):\n",
        "  pTilde = min(pdim,ndf)\n",
        "  gamma = pdim/ndf\n",
        "  ev = np.sort(eigs)\n",
        "  ind = list(range(int(alpha*pTilde), int((1-alpha)*pTilde)))\n",
        "  num =0\n",
        "  q = [qmp(i/pTilde, ndf, pdim, 1) for i in ind]\n",
        "  lamda = [ev[i] for i in ind]\n",
        "  num = np.dot(q, lamda)\n",
        "  denum = np.dot(q,q)\n",
        "  sigma_sq = num/denum\n",
        "  tw1 = TracyWidom(beta=1)\n",
        "  t_b =tw1.cdfinv(1-beta)\n",
        "  lamda_plus = sigma_sq*(((1+np.sqrt(gamma))**2+t_b*ndf**(-2/3)*(gamma)**(-1/6)*(1+np.sqrt(gamma))**4/3))\n",
        "  l2 = sigma_sq* (1+np.sqrt(gamma))**2\n",
        "  return sigma_sq, lamda_plus, l2\n",
        "\n",
        "\n",
        "#use this function to compute bema\n",
        "def bema_mat_wrapper(matrix,pReal,nReal, alpha, beta, goodnessOfFitCutoff, show = False):\n",
        "    #this block uses the fact that eigenvalues are invariant under transposition\n",
        "    #and hence without loss of generality our input matrix is p x n where\n",
        "    #p <= n. This is used to ensure that our matrix has all positive singular values\n",
        "    if pReal <= nReal:\n",
        "      p = pReal\n",
        "      n = nReal\n",
        "      matrix_norm = np.matmul(matrix, matrix.transpose())/nReal\n",
        "    else:\n",
        "      p = nReal\n",
        "      n = pReal\n",
        "      matrix_norm = np.matmul(matrix.transpose(), matrix)/nReal\n",
        "\n",
        "    v = np.linalg.eigvalsh(matrix_norm)\n",
        "    sigma_sq, lamda_plus, l2 = bema_inside(p,n,v, alpha, beta)\n",
        "    pTilde = min(p,n)\n",
        "    LinfError = error(v,alpha, pTilde, p/n, sigma_sq)\n",
        "    gamma = p/n\n",
        "    goodFit = True if LinfError < goodnessOfFitCutoff else False\n",
        "    if show:\n",
        "      print(\"error\", LinfError)\n",
        "      plt.hist(v[-min(p,n):], bins = 100, color=\"black\", label = \"Empirical Density\", density = True)\n",
        "      #plt.axvline(x=lamda_plus, label = \"Predicted Lambda Plus\", color = \"blue\")\n",
        "      Z = v[-min(p,n):]\n",
        "      for t in range(len(Z)):\n",
        "        if Z[t] > lamda_plus:\n",
        "          Z = Z[:t]\n",
        "          break\n",
        "      Y = MP_Density_Wrapper(gamma, sigma_sq, Z)\n",
        "      #plt.plot(Z,Y, color = \"orange\", label = \"Predicted Density\")\n",
        "      plt.axvline(x = lamda_plus, label = \"Lambda Plus\", color = \"red\")\n",
        "      plt.legend()\n",
        "      plt.title(\"Empirical Distribution Density\")\n",
        "      plt.show()\n",
        "\n",
        "      eigsTruncated = [i for i in v[-min(p,n):] if i < lamda_plus]\n",
        "      plt.hist(eigsTruncated, bins = 100, color = \"black\", label = \"Truncated Empirical Density\", density = True)\n",
        "      plt.plot(Z,Y, color = \"orange\", label = \"Predicted Density\")\n",
        "      plt.legend()\n",
        "      plt.title(\"Density Comparison Zoomed\")\n",
        "      plt.show()\n",
        "\n",
        "    return v,p/n,sigma_sq, lamda_plus, goodFit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIYuyPzZCjpK"
      },
      "source": [
        "The next block contains all the code necessary for computing how well the MP distribution predicted by BEMA approximates the empirical data, through computing the L-infinity norm between the theoretical CDF and empirical CDF on the range sampled by BEMA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "379shlOpBk27"
      },
      "outputs": [],
      "source": [
        "#helper MP density function evaluated at x\n",
        "def MP_Density_Inner(gamma, sigma_sq,x):\n",
        "  lp = sigma_sq*pow(1+math.sqrt(gamma),2)\n",
        "  lm = sigma_sq*pow(1-math.sqrt(gamma),2)\n",
        "  dv = math.sqrt((lp-x)*(x-lm))/(gamma*x*2*math.pi*sigma_sq)\n",
        "  return dv\n",
        "\n",
        "#at the sampled points, compute the MP distribution density\n",
        "def MP_Density_Wrapper(gamma,sigma_sq,samplePoints):\n",
        "  lp = sigma_sq*pow(1+math.sqrt(gamma),2)\n",
        "  lm = sigma_sq*pow(1-math.sqrt(gamma),2)\n",
        "\n",
        "  y = []\n",
        "  for i in samplePoints:\n",
        "    if lm <= i and i <= lp:\n",
        "      y.append(MP_Density_Inner(gamma, sigma_sq,i))\n",
        "    else: y.append(0)\n",
        "  return np.array(y)\n",
        "\n",
        "#helper function to compute MP CDF\n",
        "def MP_CDF_inner(gamma, sigma_sq, x):\n",
        "  lp = sigma_sq*pow(1+math.sqrt(gamma),2)\n",
        "  lm = sigma_sq*pow(1-math.sqrt(gamma),2)\n",
        "  r = math.sqrt((lp - x)/(x - lm))\n",
        "\n",
        "  F = math.pi * gamma + (1/sigma_sq)*math.sqrt((lp - x)* (x - lm))\n",
        "  F += -(1+gamma)*math.atan((r*r-1)/(2*r))\n",
        "  if gamma !=  1:\n",
        "    F += (1-gamma) *math.atan((lm *r*r - lp)/(2 *sigma_sq *(1-gamma)*r))\n",
        "  F /= 2 * math.pi * gamma\n",
        "  return F\n",
        "\n",
        "#at the sample points compute the theoretical MP CDF\n",
        "def MP_CDF(gamma, sigma_sq, samplePoints):\n",
        "  lp = sigma_sq*pow(1+math.sqrt(gamma),2)\n",
        "  lm = sigma_sq*pow(1-math.sqrt(gamma),2)\n",
        "\n",
        "  output = []\n",
        "  for x in samplePoints:\n",
        "    if gamma <= 1:\n",
        "      if x < lm:\n",
        "        output.append(0)\n",
        "      elif x >= lp:\n",
        "        output.append(0)\n",
        "      else:\n",
        "        output.append(MP_CDF_inner(gamma, sigma_sq,x))\n",
        "    else:\n",
        "      if x < lm:\n",
        "        output.append( (gamma-1)/gamma)\n",
        "      elif x >= lp:\n",
        "        output.append(1)\n",
        "      else:\n",
        "        output.append((gamma-1)/(2*gamma)+ MP_CDF_inner(gamma, sigma_sq, x))\n",
        "  return np.array(output)\n",
        "\n",
        "\n",
        "def empiricalCDF(S):\n",
        "  return np.array( [( i)/len(S) for i in range(len(S))])\n",
        "\n",
        "\n",
        "def error(singular_values,alpha,pTilde, gamma, sigma_sq, show = False):\n",
        "  pTilde = len(singular_values)\n",
        "  ind = np.arange(int(alpha*pTilde), int((1-alpha)*pTilde))\n",
        "  prunedSingularValues = singular_values[ind]\n",
        "  theoretical = MP_CDF(gamma, sigma_sq, prunedSingularValues)\n",
        "  empirical = alpha + (1-2*alpha)*empiricalCDF(prunedSingularValues)\n",
        "  difference = theoretical - empirical\n",
        "  if show:\n",
        "    plt.hist(difference, label = \"Difference histogram\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    x = np.arange(len(empirical))\n",
        "    plt.plot(x, empirical, label = \"empirical\")\n",
        "    plt.plot(x, theoretical, label = \"theoretical\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "  return np.linalg.norm(difference, np.inf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcPpiU2WxRNs"
      },
      "outputs": [],
      "source": [
        "def bema_scheduler(epoch):\n",
        "   return max(0, -1/300*epoch + 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aCLdlnxxxOq"
      },
      "outputs": [],
      "source": [
        "class Splittable(nn.Module):\n",
        "    \"\"\"Custom Linear layer but mimics a standard linear layer\"\"\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer2(self.layer1(x))\n",
        "\n",
        "    @property\n",
        "    def goodnessOfFitCutoff(self):\n",
        "        return self.goodnessOfFitCutoff_\n",
        "\n",
        "    @property\n",
        "    def param_numbers(self):\n",
        "        if self.splitted:\n",
        "            return (self.in_features + self.out_features) * self.layer1.out_features\n",
        "        return self.in_features * self.out_features\n",
        "\n",
        "\n",
        "    def fit_MP(self, U, singular_values, V, save_name,show = False):\n",
        "        eigenvals = singular_values**2 / V.shape[0]\n",
        "        eigenvals = np.sort(eigenvals)\n",
        "\n",
        "        p = min(U.shape[0], V.shape[0])\n",
        "        n = max(U.shape[0], V.shape[0])\n",
        "        gamma = p / n\n",
        "        sigma_sq, lamda_plus, l2 = bema_inside(p, n, eigenvals, self.alpha, self.beta)\n",
        "        Splus = math.sqrt(V.shape[1] * lamda_plus)\n",
        "        LinfError = error(eigenvals, self.alpha, p, gamma, sigma_sq)\n",
        "        goodFit = LinfError < self.goodnessOfFitCutoff\n",
        "        if show:\n",
        "          v = eigenvals\n",
        "          print(\"error\", LinfError)\n",
        "          plt.hist(v[-min(p,n):], bins = 100, color=\"black\", label = \"Empirical Density\", density = True)\n",
        "          #plt.axvline(x=lamda_plus, label = \"Predicted Lambda Plus\", color = \"blue\")\n",
        "          Z = v[-min(p,n):]\n",
        "          for t in range(len(Z)):\n",
        "            if Z[t] > lamda_plus:\n",
        "              Z = Z[:t]\n",
        "              break\n",
        "          Y = MP_Density_Wrapper(gamma, sigma_sq, Z)\n",
        "          #plt.plot(Z,Y, color = \"orange\", label = \"Predicted Density\")\n",
        "          plt.axvline(x = lamda_plus, label = \"Lambda Plus\", color = \"red\")\n",
        "          plt.legend()\n",
        "          plt.title(\"Empirical Distribution Density\")\n",
        "          plt.show()\n",
        "\n",
        "          eigsTruncated = [i for i in v[-min(p,n):] if i < lamda_plus]\n",
        "          plt.hist(eigsTruncated, bins = 100, color = \"black\", label = \"Truncated Empirical Density\", density = True)\n",
        "          plt.plot(Z,Y, color = \"orange\", label = \"Predicted Density\")\n",
        "          plt.legend()\n",
        "          plt.title(\"Density Comparison Zoomed\")\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "        return Splus, goodFit\n",
        "\n",
        "    def split(self, ratio, save_name, show = False, layer_type = 'fc'):\n",
        "        matrix = self.get_matrix()\n",
        "        U, S, V = np.linalg.svd(matrix)\n",
        "        Splus, goodFit = self.fit_MP(U, S, V, save_name)\n",
        "        if not goodFit:\n",
        "            return f\" {self.name} no good fit\"\n",
        "\n",
        "        significant_singulars = np.sum(S > Splus)\n",
        "        inner_dim = (\n",
        "            int((S.shape[0] - significant_singulars) * ratio) + significant_singulars\n",
        "        )\n",
        "        if self.param_numbers <= (matrix.shape[0] + matrix.shape[1]) * inner_dim:\n",
        "            if not self.splitted:\n",
        "                new_weights = (U[:, :inner_dim] * S[None, :inner_dim]) @ V[:inner_dim, :]\n",
        "                self.set_params(\n",
        "                    \"layer1\",\n",
        "                    torch.from_numpy(new_weights).float(),\n",
        "                    bias=None,\n",
        "                    change_bias=False,\n",
        "                )\n",
        "            return f\" {self.name} not enough param reduc\"\n",
        "\n",
        "        new_weights1 = np.sqrt(S)[:inner_dim, None] * V[:inner_dim, :]\n",
        "        new_weights2 = U[:, :inner_dim] * np.sqrt(S)[None, :inner_dim]\n",
        "        try:\n",
        "            bias = nn.Parameter(self.layer1.bias.clone())\n",
        "        except AttributeError:\n",
        "            bias = None\n",
        "        self.layer1, self.layer2 = self.make_splitted_layers(inner_dim)\n",
        "        self.set_params(\"layer1\", torch.from_numpy(new_weights1).float(), bias=None)\n",
        "        self.set_params(\"layer2\", torch.from_numpy(new_weights2).float(), bias)\n",
        "        self.splitted = True\n",
        "        return f\" {self.name} splitted, new dims {(self.in_features,inner_dim,self.out_features)}\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SplittableLinear(Splittable):\n",
        "    \"\"\"Custom Linear layer but mimics a standard linear layer\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_features,\n",
        "        out_features,\n",
        "        alpha,\n",
        "        beta,\n",
        "        goodnessOfFitCutoff,\n",
        "        name=\"splittable_linear\",\n",
        "        bias=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.layer1 = nn.Linear(in_features, out_features, bias=bias)\n",
        "        self.layer2 = nn.Identity()\n",
        "        self.splitted = False\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.goodnessOfFitCutoff_ = goodnessOfFitCutoff\n",
        "        self.name = name\n",
        "\n",
        "        # Initialize layer1 weights with N(0, 1/N)\n",
        "        nn.init.normal_(self.layer1.weight, mean=0, std=np.sqrt(1.0/in_features))\n",
        "        if bias:\n",
        "            nn.init.zeros_(self.layer1.bias)\n",
        "\n",
        "        # Initialize the bias for the second submatrix to zero\n",
        "        if hasattr(self.layer2, 'bias'):\n",
        "            nn.init.zeros_(self.layer2.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.splitted:\n",
        "            x = self.layer2(self.layer1(x))\n",
        "        else:\n",
        "            x = self.layer1(x)\n",
        "        return x\n",
        "\n",
        "    @property\n",
        "    def param_numbers(self):\n",
        "        if self.splitted:\n",
        "            return (self.in_features + self.out_features) * self.layer1.out_features\n",
        "        return self.in_features * self.out_features\n",
        "\n",
        "    def from_layer(linear, alpha, beta, goodnessOfFitCutoff):\n",
        "        bias = linear.bias != None\n",
        "        splittable_lin = SplittableLinear(\n",
        "            linear.in_features,\n",
        "            linear.out_features,\n",
        "            alpha,\n",
        "            beta,\n",
        "            goodnessOfFitCutoff,\n",
        "            bias=bias,\n",
        "        )\n",
        "        splittable_lin.set_params(\"layer1\", linear.weight, linear.bias)\n",
        "        return splittable_lin\n",
        "\n",
        "    def __str__(self):\n",
        "        return (\n",
        "            f\"Linear(in_features={self.in_features}, out_features={self.out_features})\"\n",
        "        )\n",
        "\n",
        "    def get_matrix(self):\n",
        "        layerMatrix = torch.as_tensor(self.layer1.weight)\n",
        "        layerMatrix = layerMatrix.cpu()\n",
        "        layerMatrix = layerMatrix.detach().numpy()\n",
        "        if not self.splitted:\n",
        "            return layerMatrix\n",
        "        layerMatrix2 = torch.as_tensor(self.layer2.weight)\n",
        "        layerMatrix2 = layerMatrix2.cpu()\n",
        "        layerMatrix2 = layerMatrix2.detach().numpy()\n",
        "        return layerMatrix2 @ layerMatrix\n",
        "\n",
        "    def set_params(self, which_layer, weight, bias, change_bias=True):\n",
        "        assert which_layer in [\"layer1\", \"layer2\"]\n",
        "        getattr(self, which_layer).weight = nn.Parameter(weight)\n",
        "        if change_bias:\n",
        "            if bias is None:\n",
        "                getattr(self, which_layer).bias = None\n",
        "            else:\n",
        "                getattr(self, which_layer).bias = nn.Parameter(bias)\n",
        "\n",
        "    def make_splitted_layers(self, inner_dim):\n",
        "        layer1 = nn.Linear(self.in_features, inner_dim, bias=False)\n",
        "        layer2 = nn.Linear(inner_dim, self.out_features, bias=False)\n",
        "        return layer1, layer2\n",
        "\n",
        "\n",
        "class SplittableConv(Splittable):\n",
        "    \"\"\"Custom Linear layer but mimics a standard linear layer\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size,\n",
        "        alpha,\n",
        "        beta,\n",
        "        goodnessOfFitCutoff,\n",
        "        name=\"splittable_conv\",\n",
        "        stride=1,\n",
        "        padding=0,\n",
        "        groups=1,\n",
        "        bias=True,\n",
        "        padding_mode=\"zeros\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.in_features = kernel_size * kernel_size * in_channels\n",
        "        self.out_features = out_channels\n",
        "        self.stride = stride\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "        self.groups = groups\n",
        "        self.padding_mode = padding_mode\n",
        "\n",
        "        self.layer1 = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride,\n",
        "            padding,\n",
        "            dilation=1,\n",
        "            groups=groups,\n",
        "            bias=bias,\n",
        "            padding_mode=padding_mode,\n",
        "        )\n",
        "        self.layer2 = nn.Identity()\n",
        "        self.splitted = False\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.goodnessOfFitCutoff_ = goodnessOfFitCutoff\n",
        "        self.name = name\n",
        "\n",
        "    @property\n",
        "    def param_numbers(self):\n",
        "        if self.splitted:\n",
        "            return np.prod(self.layer1.weight.shape)+np.prod(self.layer2.weight.shape)\n",
        "        return self.in_features * self.out_features\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"SplittableConv2d({self.in_channels},{self.out_channels},kernel_size=({self.kernel_size},{self.kernel_size}),stride=({self.stride},{self.stride}))\"\n",
        "\n",
        "    def from_layer(conv, alpha, beta, goodnessOfFitCutoff):\n",
        "        in_channels = conv.in_channels\n",
        "        out_channels = conv.out_channels\n",
        "        assert conv.kernel_size[0] == conv.kernel_size[1]\n",
        "        kernel_size = conv.kernel_size[0]\n",
        "        assert conv.stride[0] == conv.stride[1]\n",
        "        stride = conv.stride[0]\n",
        "        assert conv.padding[0] == conv.padding[1]\n",
        "        padding = conv.padding[0]\n",
        "        groups = conv.groups\n",
        "        padding_mode = conv.padding_mode\n",
        "        bias = conv.bias != None\n",
        "        splittable_conv = SplittableConv(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            alpha,\n",
        "            beta,\n",
        "            goodnessOfFitCutoff,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            groups=groups,\n",
        "            bias=bias,\n",
        "            padding_mode=padding_mode,\n",
        "        )\n",
        "        splittable_conv.set_params(conv.weight, conv.bias)\n",
        "        return splittable_conv\n",
        "\n",
        "    def get_matrix(self):\n",
        "        layerMatrix = torch.as_tensor(self.layer1.weight)\n",
        "        layerMatrix = layerMatrix.cpu()\n",
        "        layerMatrix = layerMatrix.detach().numpy()\n",
        "        layerMatrix = np.reshape(layerMatrix, (layerMatrix.shape[0], -1))\n",
        "        if not self.splitted:\n",
        "           return layerMatrix\n",
        "        layerMatrix2 = torch.as_tensor(self.layer2.weight)\n",
        "        layerMatrix2 = layerMatrix2.cpu()\n",
        "        layerMatrix2 = layerMatrix2.detach().numpy()\n",
        "        layerMatrix2 = np.reshape(layerMatrix2, (layerMatrix2.shape[0], -1))\n",
        "        return layerMatrix2 @ layerMatrix\n",
        "\n",
        "    def set_params(self,which_layer, weight, bias, change_bias=True):\n",
        "        assert which_layer in [\"layer1\", \"layer2\"]\n",
        "        if len(weight.shape) == 2:\n",
        "            weight = torch.reshape(\n",
        "                weight,\n",
        "                getattr(self, which_layer).weight.shape\n",
        "            )\n",
        "        getattr(self, which_layer).weight = nn.Parameter(weight)\n",
        "        if change_bias:\n",
        "            if bias is None:\n",
        "                getattr(self, which_layer).bias = None\n",
        "            else:\n",
        "                getattr(self, which_layer).bias = nn.Parameter(bias)\n",
        "\n",
        "    def make_splitted_layers(self, inner_dim):\n",
        "        layer1 = nn.Conv2d(\n",
        "            self.in_channels,\n",
        "            inner_dim,\n",
        "            self.kernel_size,\n",
        "            stride=self.stride,\n",
        "            padding=self.padding,\n",
        "            groups=self.groups,\n",
        "            bias=False,\n",
        "            padding_mode=self.padding_mode,\n",
        "        )\n",
        "        layer2 = nn.Conv2d(\n",
        "            inner_dim,\n",
        "            self.out_channels,\n",
        "            1,\n",
        "            stride=1,\n",
        "            padding=0,\n",
        "            groups=1,\n",
        "            bias=True,\n",
        "            padding_mode=self.padding_mode,\n",
        "        )\n",
        "        return layer1, layer2\n",
        "\n",
        "\n",
        "class SplittableLayer(nn.Module):\n",
        "    \"\"\"Custom Linear layer but mimics a standard linear layer\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim,\n",
        "        out_dim,\n",
        "        alpha,\n",
        "        beta,\n",
        "        goodnessOfFitCutoff,\n",
        "        name = \"splittable_linear\",\n",
        "        bias = True,\n",
        "        layer_type = 'fc',\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if layer_type == \"fc\":\n",
        "          self.in_features = in_dim\n",
        "          self.out_features = out_dim\n",
        "          self.layer1 = nn.Linear(in_dim, out_dim, bias=bias)\n",
        "\n",
        "        if layer_type == \"conv\":\n",
        "          self.in_channels = in_dim\n",
        "          self.out_channels = out_dim\n",
        "          self.layer1 = nn.Conv2d(in_dim, out_dim, 3, bias=bias)\n",
        "\n",
        "        self.layer2 = nn.Identity()\n",
        "        self.splitted = False\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.goodnessOfFitCutoff_ = goodnessOfFitCutoff\n",
        "        self.name = name\n",
        "        self.layer_type = layer_type\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layer2(self.layer1(x))\n",
        "\n",
        "    @property\n",
        "    def goodnessOfFitCutoff(self):\n",
        "        return self.goodnessOfFitCutoff_\n",
        "\n",
        "    @property\n",
        "    def param_numbers(self):\n",
        "        if self.splitted:\n",
        "            return (self.in_features + self.out_features) * self.layer1.out_features\n",
        "        return self.in_features * self.out_features\n",
        "\n",
        "    @property\n",
        "    def param_numbers(self):\n",
        "        if self.splitted:\n",
        "            return (self.in_features + self.out_features) * self.layer1.out_features\n",
        "        return self.in_features * self.out_features\n",
        "\n",
        "\n",
        "    def fit_MP(self, U, singular_values, V, save_name,show = False):\n",
        "        eigenvals = singular_values**2 / V.shape[0]\n",
        "        eigenvals = np.sort(eigenvals)\n",
        "\n",
        "        p = min(U.shape[0], V.shape[0])\n",
        "        n = max(U.shape[0], V.shape[0])\n",
        "        gamma = p / n\n",
        "        sigma_sq, lamda_plus, l2 = bema_inside(p, n, eigenvals, self.alpha, self.beta)\n",
        "        Splus = math.sqrt(V.shape[1] * lamda_plus)\n",
        "        LinfError = error(eigenvals, self.alpha, p, gamma, sigma_sq)\n",
        "        goodFit = LinfError < self.goodnessOfFitCutoff\n",
        "        if show:\n",
        "          v = eigenvals\n",
        "          print(\"error\", LinfError)\n",
        "          plt.hist(v[-min(p,n):], bins = 100, color=\"black\", label = \"Empirical Density\", density = True)\n",
        "          #plt.axvline(x=lamda_plus, label = \"Predicted Lambda Plus\", color = \"blue\")\n",
        "          Z = v[-min(p,n):]\n",
        "          for t in range(len(Z)):\n",
        "            if Z[t] > lamda_plus:\n",
        "              Z = Z[:t]\n",
        "              break\n",
        "          Y = MP_Density_Wrapper(gamma, sigma_sq, Z)\n",
        "          #plt.plot(Z,Y, color = \"orange\", label = \"Predicted Density\")\n",
        "          plt.axvline(x = lamda_plus, label = \"Lambda Plus\", color = \"red\")\n",
        "          plt.legend()\n",
        "          plt.title(\"Empirical Distribution Density\")\n",
        "          plt.show()\n",
        "\n",
        "          eigsTruncated = [i for i in v[-min(p,n):] if i < lamda_plus]\n",
        "          plt.hist(eigsTruncated, bins = 100, color = \"black\", label = \"Truncated Empirical Density\", density = True)\n",
        "          plt.plot(Z,Y, color = \"orange\", label = \"Predicted Density\")\n",
        "          plt.legend()\n",
        "          plt.title(\"Density Comparison Zoomed\")\n",
        "          plt.show()\n",
        "\n",
        "\n",
        "        return Splus, goodFit\n",
        "\n",
        "    def split(self, ratio, save_name, show = False, layer_type = 'fc'):\n",
        "        matrix = self.get_matrix()\n",
        "        U, S, V = np.linalg.svd(matrix)\n",
        "\n",
        "        if layer_type == \"fc\":\n",
        "          Splus, goodFit = self.fit_MP(U, S, V, save_name, show = show)\n",
        "        if layer_type == \"conv\":\n",
        "          Splus, goodFit = self.fit_MP(U, np.array(S).flatten(), V, save_name, show = show)\n",
        "\n",
        "        if not goodFit:\n",
        "            return f\" {self.name} not good fit\"\n",
        "        #print(S.shape,\"qrwetesydryj\")\n",
        "        significant_singulars = np.sum(np.array(S) > Splus)\n",
        "        #print()\n",
        "        if layer_type == \"fc\":\n",
        "          inner_dim = (\n",
        "              int((S.shape[0] - significant_singulars) * ratio) + significant_singulars\n",
        "          )\n",
        "        if layer_type == \"conv\":\n",
        "          inner_dim = (\n",
        "              int((S.shape[1] - significant_singulars) * ratio) + significant_singulars\n",
        "          )\n",
        "        old_num_param = np.prod(matrix.shape)\n",
        "        print(old_num_param , matrix.shape, inner_dim, significant_singulars, \"erqwt\")\n",
        "        print()\n",
        "        if self.param_numbers <= (matrix.shape[0] + matrix.shape[1]) * inner_dim:\n",
        "            if layer_type == \"fc\":\n",
        "              new_weights = (U[:, :inner_dim] * S[None, :inner_dim]) @ V[:inner_dim, :]\n",
        "            if layer_type ==\"conv\":\n",
        "              new_weights = (U[:,:inner_dim, :,:] * S[None,:inner_dim,:,:]) @ V [:,:inner_dim,:, :]\n",
        "              #(U[..., :3] * S[..., None,:]) @ V [:inner_dim, :]\n",
        "\n",
        "            self.set_params(\n",
        "                'layer1',torch.from_numpy(new_weights).float(), bias=None, change_bias=False\n",
        "            )\n",
        "            return f\" {self.name} not enough param reduc\"\n",
        "        print(U.shape, S.shape, V.shape,\"10293847\")\n",
        "        if layer_type == \"fc\":\n",
        "          print(U[:, :inner_dim].shape, np.sqrt(S)[None, :inner_dim].shape, np.sqrt(S)[:inner_dim, None].shape , V[:inner_dim, :].shape,\"pppp\")\n",
        "          new_weights1 = np.sqrt(S)[:inner_dim, None] * V[:inner_dim, :]\n",
        "          new_weights2 = U[:, :inner_dim] * np.sqrt(S)[None, :inner_dim]\n",
        "          print(new_weights1.shape, new_weights2.shape,\" fc new\")\n",
        "\n",
        "        if layer_type == \"conv\":\n",
        "          print(U.shape, S.shape, V.shape, \"gjhgjhg\")\n",
        "          print(U[:,:inner_dim,:,:].shape, np.sqrt(S)[None,:inner_dim,:3,:3].shape, np.sqrt(S)[:inner_dim,None,:3,:3].shape, V[:inner_dim,:, :,:].shape,\"asfd\")\n",
        "          new_weights1 =  np.sqrt(S)[:inner_dim,None,:3,:3] * V[:inner_dim,:, :,:]\n",
        "          #np.sqrt(S)[..., None,:] * V#[:3, :]\n",
        "          new_weights2 = U[:,:inner_dim,:,:] * np.sqrt(S)[None,:inner_dim,:3,:3]\n",
        "          #U[..., ,:3] * np.sqrt(S)[None,:inner_dim,:3,:3]\n",
        "          print(new_weights1.shape, new_weights2.shape, \" conv new\")\n",
        "        try:\n",
        "            bias = nn.Parameter(self.layer1.bias.clone())\n",
        "        except AttributeError:\n",
        "            bias = None\n",
        "        self.layer1, self.layer2 = self.make_splitted_layers(inner_dim)\n",
        "        self.set_params(\"layer1\", torch.from_numpy(new_weights1).float(), bias=None)\n",
        "        self.set_params(\"layer2\", torch.from_numpy(new_weights2).float(), bias)\n",
        "        self.splitted = True\n",
        "        if layer_type == \"fc\":\n",
        "          return f\" {self.name} splitted, new dims {(self.in_features, inner_dim, self.out_features)}\"\n",
        "        if layer_type == \"conv\":\n",
        "          return f\" {self.name} splitted, new dims {(self.in_channels, inner_dim, self.out_channels)}\"\n",
        "\n",
        "    def from_layer(self, layer, alpha, beta, goodnessOfFitCutoff):\n",
        "        bias = layer.bias != None\n",
        "\n",
        "        if self.layer_type == 'fc':\n",
        "          in_dim = layer.in_features\n",
        "          out_dim = layer.out_features\n",
        "\n",
        "        if self.layer_type == 'conv':\n",
        "          in_dim = layer.in_channels\n",
        "          out_dim = layer.in_channels\n",
        "\n",
        "        splittable_layer = SplittableLayer(\n",
        "            in_dim,\n",
        "            out_dim,\n",
        "            alpha,\n",
        "            beta,\n",
        "            goodnessOfFitCutoff,\n",
        "            bias=bias,\n",
        "        )\n",
        "        splittable_layer.set_params(\"layer1\", layer.weight, layer.bias)\n",
        "        return splittable_layer\n",
        "\n",
        "    def __str__(self):\n",
        "      if self.layer_type == 'fc':\n",
        "        return (f\"Linear(in_features={self.in_features}, out_features={self.out_features})\")\n",
        "      if self.layer_type == 'conv':\n",
        "        return (f\"Convolutional(in_channels={self.in_channels}, out_channels={self.out_channels})\")\n",
        "\n",
        "    def get_matrix(self):\n",
        "        layerMatrix = torch.as_tensor(self.layer1.weight)\n",
        "        layerMatrix = layerMatrix.cpu()\n",
        "        layerMatrix = layerMatrix.detach().numpy()\n",
        "        if not self.splitted:\n",
        "            return layerMatrix\n",
        "        layerMatrix2 = torch.as_tensor(self.layer2.weight)\n",
        "        layerMatrix2 = layerMatrix2.cpu()\n",
        "        layerMatrix2 = layerMatrix2.detach().numpy()\n",
        "        return layerMatrix2 @ layerMatrix\n",
        "\n",
        "    def set_params(self, which_layer, weight, bias, change_bias=True):\n",
        "        assert which_layer in [\"layer1\", \"layer2\"]\n",
        "        getattr(self, which_layer).weight = nn.Parameter(weight)\n",
        "        if change_bias:\n",
        "            if bias is None:\n",
        "                getattr(self, which_layer).bias = None\n",
        "            else:\n",
        "                getattr(self, which_layer).bias = nn.Parameter(bias)\n",
        "\n",
        "    def make_splitted_layers(self, inner_dim):\n",
        "        if self.layer_type == 'fc':\n",
        "          layer1 = nn.Linear(self.in_features, inner_dim, bias=False)\n",
        "          layer2 = nn.Linear(inner_dim, self.out_features, bias=False)\n",
        "        if self.layer_type == 'conv':\n",
        "          layer1 = nn.Conv2d(self.in_channels, inner_dim, 3, bias=False)\n",
        "          layer2 = nn.Conv2d(inner_dim, self.out_channels, 3, bias=False)\n",
        "        return layer1, layer2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvUIWKqItbar"
      },
      "source": [
        "the following block contains the splitting algorithms that process one layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-kMKRAytaGX"
      },
      "outputs": [],
      "source": [
        "#interface to the rest of the code that processes\n",
        "def innerAlgWrapper(model, index, lambda_plus, eigsToKeep, goodFit, alg = 0):\n",
        "  if alg == -1:\n",
        "    return model, 0\n",
        "  if alg == 0:\n",
        "    return algZero(model, index, lambda_plus, eigsToKeep, goodFit)\n",
        "  elif alg == 1:\n",
        "    return algOne(model,index,lambda_plus,eigsToKeep,goodFit)\n",
        "  elif alg == 2:\n",
        "    return algTwo(model,index,lambda_plus,eigsToKeep,goodFit)\n",
        "  elif alg == 3:\n",
        "    return algThree(model, index, lambda_plus,eigsToKeep,goodFit)\n",
        "  elif alg == 4:\n",
        "    return algFour(model, index, lambda_plus, eigsToKeep, goodFit)\n",
        "\n",
        "\n",
        "\n",
        "#split + truncate, only split if reduces size\n",
        "#no modifications if not good fit\n",
        "def algZero(model, index, lambda_plus, eigsToKeep, goodFit):\n",
        "  if not goodFit: return model, 0\n",
        "\n",
        "  dims = model.getDimsFC()\n",
        "  layerMatrix = model.getLayerMatrix(index)\n",
        "  #layerMatrix has size outputDim x inputDim\n",
        "  #so layer 1 will have eigsToKeep x inputDim\n",
        "  #layer 2 will have outputDim x eigsToKeep\n",
        "  inputDim = dims[index]\n",
        "  outputDim = dims[index+1]\n",
        "\n",
        "  (U,S,V) = np.linalg.svd(layerMatrix)\n",
        "  s = np.zeros([outputDim, inputDim])\n",
        "\n",
        "  for i in range(0, eigsToKeep):\n",
        "    s[i][i] = S[i]**(1/2)\n",
        "\n",
        "  #the flip here comes from pytorch not computing Wx + b\n",
        "  #instead it computes x W^T + b\n",
        "  #thus if W = W2 W1 then we want W1 then W2 so that\n",
        "  #x W^T + b = x W1^T W2^T + b\n",
        "  w2 = np.matmul(U,s)[:,:eigsToKeep]\n",
        "  w1 = np.matmul(s,V)[:eigsToKeep,:]\n",
        "  withoutrel = model.getWithout()\n",
        "\n",
        "  #only split if this lowers total param numbers\n",
        "  capChange = 0\n",
        "  if inputDim*outputDim < eigsToKeep * (inputDim + outputDim):\n",
        "    model.fc[index].weight = torch.nn.Parameter(torch.from_numpy(w2 @ w1).float())\n",
        "  else:\n",
        "    capChange = 1\n",
        "\n",
        "    layerOne = nn.Linear(eigsToKeep,outputDim, bias=False)\n",
        "    layerOne.weight = torch.nn.Parameter(torch.from_numpy(w1).float())\n",
        "\n",
        "    if model.fc[index].bias is not None:\n",
        "      layerTwo = nn.Linear(inputDim, eigsToKeep, bias=True)\n",
        "      layerTwo.bias = model.fc[index].bias.float()\n",
        "    else:\n",
        "      layerTwo = nn.Linear(inputDim, eigsToKeep, bias=False)\n",
        "\n",
        "    layerTwo.weight = torch.nn.Parameter(torch.from_numpy(w2).float())\n",
        "    model.fc[index] = layerOne\n",
        "    model.fc.insert(index+1, layerTwo)\n",
        "    model.without_rel.insert(index, True)\n",
        "    model.dims.insert(index+1, eigsToKeep)\n",
        "  return model, capChange\n",
        "\n",
        "#truncate spectrum at eigstokeep if goodfit\n",
        "#no split\n",
        "def algOne(model, index, lambda_plus, eigsToKeep, goodFit):\n",
        "  if not goodFit: return model, 0\n",
        "\n",
        "  dims = model.getDims()\n",
        "  layerMatrix = model.getLayerMatrix(index)\n",
        "\n",
        "  #layerMatrix has size outputDim x inputDim\n",
        "  inputDim = dims[index]\n",
        "  outputDim = dims[index+1]\n",
        "\n",
        "  (U,S,V) = np.linalg.svd(layerMatrix)\n",
        "  s = np.zeros([outputDim, inputDim])\n",
        "\n",
        "  for i in range(eigsToKeep):\n",
        "    s[i][i] = S[i]\n",
        "  model.fc[index].weight = torch.nn.Parameter(torch.from_numpy(U @ s @ V).float())\n",
        "  return model, 0\n",
        "\n",
        "#shrink if good fit, no split ever\n",
        "def algTwo(model, index, lambda_plus, eigsToKeep, goodFit):\n",
        "  if not goodFit: return model, 0\n",
        "\n",
        "  dims = model.getDims()\n",
        "  layerMatrix = model.getLayerMatrix(index)\n",
        "\n",
        "  #layerMatrix has size outputDim x inputDim\n",
        "  inputDim = dims[index]\n",
        "  outputDim = dims[index+1]\n",
        "\n",
        "  (U,S,V) = np.linalg.svd(layerMatrix)\n",
        "  s = np.zeros([outputDim, inputDim])\n",
        "  for i in range(len(S)):\n",
        "    s[i][i] = S[i]\n",
        "    if S[i] <= lambda_plus:\n",
        "      s[i][i]*= 0.5\n",
        "\n",
        "  model.fc[index].weight = torch.nn.Parameter(torch.from_numpy(U@s@V).float())\n",
        "  return model, 0\n",
        "\n",
        "#alg3 = prune uniformly if good fit\n",
        "#split if parameters reduced\n",
        "def algThree(model, index, lambda_plus, eigsToKeep, goodFit):\n",
        "  if not goodFit: return model, 0\n",
        "\n",
        "  dims = model.getDims()\n",
        "  layerMatrix = model.getLayerMatrix(index)\n",
        "\n",
        "  #layerMatrix has size outputDim x inputDim\n",
        "  #so layer 1 will have eigsToKeep x inputDim\n",
        "  #layer 2 will have outputDim x eigsToKeep\n",
        "  inputDim = dims[index]\n",
        "  outputDim = dims[index+1]\n",
        "\n",
        "  (U,S,V) = np.linalg.svd(layerMatrix)\n",
        "  s = np.zeros([outputDim, inputDim])\n",
        "  effectiveRank = computePredictedRank(model, index)\n",
        "  for i in range(0, len(S)):\n",
        "    s[i][i] = S[i]**(1/2)\n",
        "\n",
        "  eigMask = list(range(effectiveRank))\n",
        "  sampleDomain = list(range(effectiveRank, len(S)))\n",
        "  eigsToKeepBelow = min(len(sampleDomain), eigsToKeep)\n",
        "  eigMask.extend(random.sample(sampleDomain, eigsToKeepBelow))\n",
        "  eigsToKeep = len(eigMask)\n",
        "\n",
        "  #the flip here comes from pytorch not computing Wx + b\n",
        "  #instead it computes x W^T + b\n",
        "  #thus if W = W2 W1 then we want W1 then W2 so that\n",
        "  #x W^T + b = x W1^T W2^T + b\n",
        "  w2 = U[np.ix_(range(np.shape(U)[0]),eigMask)] @ s[np.ix_(eigMask,eigMask)]\n",
        "  w1 = s[np.ix_(eigMask,eigMask)] @ V[np.ix_(eigMask,range(np.shape(V)[1]))]\n",
        "\n",
        "  #only split if this lowers total param numbers\n",
        "  capChange = 0\n",
        "  if inputDim*outputDim < len(eigMask) * (inputDim + outputDim):\n",
        "    model.fc[index].weight = torch.nn.Parameter(torch.from_numpy(w2 @ w1).float())\n",
        "  else:\n",
        "    capChange = 1\n",
        "\n",
        "    layerOne = nn.Linear(eigsToKeep,outputDim, bias=False)\n",
        "    layerOne.weight = torch.nn.Parameter(torch.from_numpy(w1).float())\n",
        "\n",
        "    if model.fc[index].bias is not None:\n",
        "      layerTwo = nn.Linear(inputDim, eigsToKeep, bias=True)\n",
        "      layerTwo.bias = model.fc[index].bias.float()\n",
        "    else:\n",
        "      layerTwo = nn.Linear(inputDim, eigsToKeep, bias=False)\n",
        "\n",
        "    layerTwo.weight = torch.nn.Parameter(torch.from_numpy(w2).float())\n",
        "    model.fc[index] = layerOne\n",
        "    model.fc.insert(index+1, layerTwo)\n",
        "    model.without_rel.insert(index, True)\n",
        "    model.dims.insert(index+1, eigsToKeep)\n",
        "  return model, capChange\n",
        "\n",
        "def algFour(model, index, lambda_plus, eigsToKeep, goodFit):\n",
        "  if not goodFit: return model, 0\n",
        "\n",
        "  dims = model.getDimsConv()\n",
        "  layerMatrix = model.getLayerMatrixOriginal(index)\n",
        "  #layerMatrix has size outputDim x inputDim\n",
        "  #so layer 1 will have eigsToKeep x inputDim\n",
        "  #layer 2 will have outputDim x eigsToKeep\n",
        "\n",
        "  outputDim, inputDim  = layerMatrix.shape[:2]\n",
        "\n",
        "  (U, S, V) = np.linalg.svd(layerMatrix)\n",
        "  s = np.zeros(layerMatrix.shape)\n",
        "\n",
        "  for k in range(layerMatrix.shape[0]):\n",
        "    for j in range(layerMatrix.shape[1]):\n",
        "      for i in range(eigsToKeep): #0 ,\n",
        "        s[k][j][i%3][i%3] = S[k][j][i%3]**(1/2)\n",
        "\n",
        "  #the flip here comes from pytorch not computing Wx + b\n",
        "  #instead it computes x W^T + b\n",
        "  #thus if W = W2 W1 then we want W1 then W2 so that\n",
        "  #x W^T + b = x W1^T W2^T + b\n",
        "  w2 = np.matmul(U,s)[:,:eigsToKeep]\n",
        "  w1 = np.matmul(s,V)[:eigsToKeep,:]\n",
        "  withoutrel = model.getWithout()[0]\n",
        "  #only split if this lowers total param numbers\n",
        "  capChange = 0\n",
        "  if inputDim*outputDim < eigsToKeep * (inputDim + outputDim):\n",
        "    model.conv[index].weight = torch.nn.Parameter(torch.from_numpy(w2 @ w1).float())\n",
        "  else:\n",
        "    capChange = 1\n",
        "\n",
        "    layerOne = nn.Conv2d(eigsToKeep, outputDim, 3, bias = False)\n",
        "    layerOne.weight = torch.nn.Parameter(torch.from_numpy(w1).float())\n",
        "\n",
        "    if model.conv[index].bias is not None:\n",
        "      layerTwo = nn.Conv2d(inputDim, eigsToKeep, 3, bias = True)\n",
        "      layerTwo.bias = model.conv[index].bias.float()\n",
        "\n",
        "    else:\n",
        "      layerTwo = nn.Conv2d(inputDim, eigsToKeep, 3, bias = False)\n",
        "\n",
        "    layerTwo.weight = torch.nn.Parameter(torch.from_numpy(w2).float())\n",
        "    model.conv[index] = layerOne\n",
        "    model.conv.insert(index+1, layerTwo)\n",
        "    model.without_rel.insert(index, True)\n",
        "    model.dims.insert(index+1, eigsToKeep)\n",
        "  return model, capChange"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFuEbTagJQT9"
      },
      "source": [
        "The following block contains all the splitting code. It takes as input the model and which layer to be split, and returns the model with the selected layer split (according to the BEMA algorithm). splitWrapper should be called whenever you want to split the network, and it will apply the SVD algorithm to every layer.\n",
        "\n",
        "The algorithm consists of the following. We find lambda+ and then keep X% of the singular values below lambda+ where the specific percentage is governed by our bema_scheduler. We split according to the square root trick when doing so reduces the total number of parameters of the network, with the caveat that we never split to a layer with size smaller than the end layer (to prevent information bottlenecks)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdHCfEIu7_am"
      },
      "outputs": [],
      "source": [
        "#computes lamdaPlus, eigsToKeep, and whether fit is good\n",
        "def computeEigsToKeep(model, layerMatrix, dims, epoch, goodnessOfFitCutoff, show = False):\n",
        "  (p,n) = layerMatrix.shape\n",
        "\n",
        "  eigs, gamma, sigma_sq,lambda_plus, goodFit = bema_mat_wrapper(layerMatrix, p, n, 0.2, 0.1, goodnessOfFitCutoff, show = show)\n",
        "\n",
        "  lt = len(eigs)-1\n",
        "  for i in range(len(eigs)):\n",
        "    if eigs[i] > lambda_plus:\n",
        "      lt = i - 1\n",
        "      break\n",
        "\n",
        "  #lt = number of eigs less than lambda_plus\n",
        "  #gt = number greater than lambda_plus\n",
        "  gt = len(eigs) - lt\n",
        "  p = gt + int(bema_scheduler(epoch)*lt)\n",
        "\n",
        "  #this line is a necessary sanity check to stop the network from\n",
        "  #pruning itself smaller than the output dimension\n",
        "  eigsToKeep = max(p, dims[-1])\n",
        "  lpTransformed = np.sqrt(lambda_plus)*np.sqrt(n)\n",
        "\n",
        "  return lpTransformed, eigsToKeep, goodFit\n",
        "\n",
        "#computes rank predicted by BEMA\n",
        "def computePredictedRank(model, index, goodnessOfFitCutoff = 0.01, show = False):\n",
        "  dims = model.getDims()\n",
        "  layerMatrix = model.getLayerMatrix(index)\n",
        "\n",
        "  (p,n) = layerMatrix.shape\n",
        "\n",
        "  eigs, gamma, sigma_sq,lambda_plus, goodFit = bema_mat_wrapper(layerMatrix, p, n, 0.25, 0.1, goodnessOfFitCutoff)\n",
        "  gt = 0\n",
        "\n",
        "\n",
        "  for i in range(len(eigs)):\n",
        "    if eigs[i] > lambda_plus:\n",
        "      gt += 1\n",
        "  if show:\n",
        "    plt.hist(eigs, bins = 200)\n",
        "    plt.axvline(x = lambda_plus)\n",
        "    plt.show()\n",
        "  return gt\n",
        "\n",
        "\n",
        "#when desired to apply split algorithm, call splitWrapper\n",
        "def splitWrapper(model, device, epoch, goodnessOfFitCutoff, show = False, alg = [0]):\n",
        "  print(\"cutting. at this epoch we keep\", bema_scheduler(epoch))\n",
        "  #caps = [dims for dims in model.getDims()]\n",
        "  #layersAdded ensures that when a layer is split, we skip over it\n",
        "  for k in range(len(alg)):\n",
        "    layersAdded = 0\n",
        "    if alg[k] == 0:\n",
        "      cap = model.getDimsFC()\n",
        "    if alg[k] == 4:\n",
        "      cap = model.getDimsConv()\n",
        "    for j in range(len(cap) - 1):\n",
        "      if alg[k] == 0:\n",
        "        layer = model.getLayerMatrixFC(j + layersAdded)\n",
        "      if alg[k] == 4:\n",
        "        layer = model.getLayerMatrixConv(j + layersAdded)\n",
        "      layer_size = np.prod(layer.shape)  # numpy.prod() gives total number of elements\n",
        "      if layer_size > 300000:\n",
        "          lpTransformed, eigsToKeep, goodFit = computeEigsToKeep(model, j+layersAdded, cap, epoch, goodnessOfFitCutoff[k], show = show)\n",
        "          model, capChange = innerAlgWrapper(model, j+layersAdded, lpTransformed, eigsToKeep, goodFit, alg = alg[k])\n",
        "          model.to(device)\n",
        "          layersAdded += capChange\n",
        "  #print(\"New Dimensions:\", model.getDims())\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gk_zVn-QY7UB"
      },
      "outputs": [],
      "source": [
        "def getMeanAndVar(yData):\n",
        "  length = len(yData[0])\n",
        "  mean = np.zeros(length)\n",
        "  squaredMean = np.zeros(length)\n",
        "\n",
        "  for i in yData:\n",
        "    mean += i\n",
        "    squaredMean += np.square(i)\n",
        "  mean = mean / len(yData)\n",
        "  squaredMean = squaredMean / len(yData)\n",
        "\n",
        "  variance = squaredMean - np.square(mean)\n",
        "  return mean, variance\n",
        "\n",
        "def graphData(xData, yData, title = None):\n",
        "  mean, variance = getMeanAndVar(yData)\n",
        "  std = np.sqrt(variance)\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  if title != None:\n",
        "    plt.title(title)\n",
        "  plt.plot(xData, mean, label = \"Mean\", color = \"black\")\n",
        "  plt.plot(xData, mean - std, \"--\", label = \"Mean - 1std\", color = \"red\")\n",
        "  plt.plot(xData, mean + std, \"--\", label = \"Mean + 1std\", color = \"blue\")\n",
        "  plt.fill_between(xData, mean, mean + std, color = \"grey\", alpha = 0.5)\n",
        "  plt.fill_between(xData, mean, mean - std, color = \"grey\", alpha = 0.5)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "def graphDataBoth(xData, yData0, yData1):\n",
        "  L = [yData0, yData1]\n",
        "  Labels = [\"Normal\", \"Pruned\"]\n",
        "  colors = [\"black\", \"red\"]\n",
        "  plt.title(\"Normal Vs Pruned Comparison\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  for i in range(2):\n",
        "    mean, variance = getMeanAndVar(L[i])\n",
        "    std = np.sqrt(variance)\n",
        "    plt.plot(xData, mean, label = \"Mean \" + Labels[i], color = colors[i])\n",
        "    plt.plot(xData, mean - std, \"--\", label = \"Mean - 1std \" +  Labels[i], color = colors[i])\n",
        "    plt.plot(xData, mean + std, \"--\", label = \"Mean + 1std \" + Labels[i], color = colors[i])\n",
        "    plt.fill_between(xData, mean, mean + std, color = \"grey\", alpha = 0.5)\n",
        "    plt.fill_between(xData, mean, mean - std, color = \"grey\", alpha = 0.5)\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHRg69NYF_H3"
      },
      "source": [
        "the following block allows for testing out the various algorithms and adjusting the hyper parameters, comparing accuracy between our modified algorithm and a stock SGD training algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXRZ1kY-2vuP",
        "outputId": "e480e90b-38a6-4131-84d1-20f277d32eff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Seed: 4435912\n",
            "Seed: 4435912\n",
            "Train set: Average loss: 2.1360, Accuracy: 49913/60000 (83%)\n",
            "test set: Average loss: 50.631303, Accuracy: 8548/10000 (85%)\n",
            "Number of parameters of DNN 20391010\n",
            "Train set: Average loss: 1.9563, Accuracy: 52685/60000 (88%)\n",
            "test set: Average loss: 44.784083, Accuracy: 8712/10000 (87%)\n",
            "Number of parameters of DNN 20391010\n",
            "Train set: Average loss: 1.8707, Accuracy: 53535/60000 (89%)\n",
            "test set: Average loss: 42.377631, Accuracy: 8785/10000 (88%)\n",
            "Number of parameters of DNN 20391010\n",
            "Train set: Average loss: 1.7950, Accuracy: 54189/60000 (90%)\n",
            "test set: Average loss: 46.568660, Accuracy: 8694/10000 (87%)\n",
            "Number of parameters of DNN 20391010\n",
            "Train set: Average loss: 1.7278, Accuracy: 54802/60000 (91%)\n",
            "test set: Average loss: 43.390534, Accuracy: 8749/10000 (87%)\n",
            "Number of parameters of DNN 20391010\n",
            "Train set: Average loss: 1.6667, Accuracy: 55169/60000 (92%)\n",
            "test set: Average loss: 40.558957, Accuracy: 8886/10000 (89%)\n",
            "Number of parameters of DNN 20391010\n",
            "Train set: Average loss: 1.6082, Accuracy: 55521/60000 (93%)\n",
            "test set: Average loss: 40.610376, Accuracy: 8861/10000 (89%)\n",
            "Number of parameters of DNN 20391010\n",
            "Train set: Average loss: 1.5526, Accuracy: 55964/60000 (93%)\n",
            "test set: Average loss: 42.701762, Accuracy: 8806/10000 (88%)\n",
            "Number of parameters of DNN 20391010\n",
            "Train set: Average loss: 1.5023, Accuracy: 56297/60000 (94%)\n",
            "test set: Average loss: 42.176710, Accuracy: 8879/10000 (89%)\n",
            "Number of parameters of DNN 20391010\n",
            "Train set: Average loss: 1.4523, Accuracy: 56548/60000 (94%)\n",
            "test set: Average loss: 40.438992, Accuracy: 8933/10000 (89%)\n",
            "Number of parameters of DNN 20391010\n",
            "Train set: Average loss: 1.4097, Accuracy: 56646/60000 (94%)\n",
            "test set: Average loss: 40.924376, Accuracy: 8869/10000 (89%)\n",
            "Number of parameters of DNN 20391010\n",
            "Train set: Average loss: 1.3614, Accuracy: 56934/60000 (95%)\n",
            "test set: Average loss: 40.986854, Accuracy: 8912/10000 (89%)\n",
            "Number of parameters of DNN 20391010\n",
            " layer 1 not enough param reduc\n",
            "Cutting. At this epoch we keep 0.9566666666666667\n",
            " layer 2 not enough param reduc\n",
            "Cutting. At this epoch we keep 0.9566666666666667\n",
            " layer 3 not enough param reduc\n",
            "Cutting. At this epoch we keep 0.9566666666666667\n",
            " layer 4 splitted, new dims (3000, 9, 10)\n",
            "Cutting. At this epoch we keep 0.9566666666666667\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Directory to save models\n",
        "save_dir = \"/content/drive/MyDrive/DNN_models_5/\"\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# Training hyperparameters\n",
        "trainCycles = 1000\n",
        "seed = 4435912\n",
        "lr = 0.02\n",
        "m = 50\n",
        "momentum = 0.9\n",
        "batchSize = 128\n",
        "splitFreq = 13\n",
        "showTrainingLoss = False\n",
        "showSpectrumDuringSplit = False\n",
        "goodnessOfFitCutoff = [1]\n",
        "\n",
        "# Network topology setup\n",
        "dims = [28*28, 3000, 3000, 3000, 10]\n",
        "no_rel = [False for i in range(len(dims)-2)]+[True,True]  # Layers with relu\n",
        "\n",
        "def count_total_params(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "def naive_prune(layer, threshold):\n",
        "    with torch.no_grad():\n",
        "        weight_mask = torch.abs(layer.weight) > threshold\n",
        "        layer.weight *= weight_mask.float()\n",
        "\n",
        "def count_nonzero_params(model):\n",
        "    count = 0\n",
        "    for param in model.parameters():\n",
        "        count += torch.count_nonzero(param).item()\n",
        "    return count\n",
        "\n",
        "# New parameters\n",
        "trialRuns = 1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    use_cuda = not False and torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu:1\")\n",
        "\n",
        "    yTestDataStock = []\n",
        "    yTestDataModified = []\n",
        "    yTrainDataStock = []\n",
        "    yTrainDataModified = []\n",
        "\n",
        "    for trainCycle in range(trialRuns):\n",
        "        seed = seed + trainCycle  # Adjust the seed calculation\n",
        "        torch.manual_seed(seed)\n",
        "        print(\"Seed:\", seed)  # Print the seed value\n",
        "\n",
        "        x, y_test, y_test2, y_train, y_train2 = [], [], [], [], []\n",
        "\n",
        "        torch.manual_seed(seed)  # Ensure `model2` gets the same initial weights as `model`\n",
        "        print(\"Seed:\", seed)  # Print the seed value\n",
        "\n",
        "        model = networkModelModified(no_rel, dims, alpha=0.25, beta=0.9, goodnessOfFitCutoff=[0])\n",
        "        model.to(device)\n",
        "        (args, model, optimizer, test_loader, train_loader) = neuralInit(seed, device, model, lr, momentum, batchSize)\n",
        "        optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "        model2 = networkModelModified(no_rel, dims, alpha=0.25, beta=0.1, goodnessOfFitCutoff=goodnessOfFitCutoff)\n",
        "        model2.to(device)\n",
        "        (args, model2, optimizer2, test_loader, train_loader) = neuralInit(seed, device, model2, lr, momentum, batchSize)\n",
        "        optimizer2 = optim.SGD(model2.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "        num_params_unpruned = sum([p.numel() for p in model2.parameters() if p.requires_grad])\n",
        "        accuracies = []\n",
        "        num_params_pruned = []\n",
        "        threshold_values = [1/(1+k) for k in range(m)]  # Example thresholds\n",
        "\n",
        "        for epoch in range(1, trainCycles + 1):\n",
        "            # Update learning rate and split at specified frequency\n",
        "            lr *= 0.96\n",
        "            if epoch % splitFreq == 0 and epoch != 0:\n",
        "                for i, layer in enumerate(model2.fc):\n",
        "                    print(layer.split(bema_scheduler(epoch), save_name=f'layer_{i}_epoch_{epoch}', show=showSpectrumDuringSplit))\n",
        "                    print(\"Cutting. At this epoch we keep\", bema_scheduler(epoch))\n",
        "                optimizer2 = optim.SGD(model2.parameters(), lr=lr, momentum=args['momentum'])\n",
        "\n",
        "            # Training and testing\n",
        "            y_train2.append(train(args, model2, device, train_loader, optimizer2, epoch, showTrainingLoss))\n",
        "            y_test2.append(test(args, model2, device, test_loader))\n",
        "            print(\"Number of parameters of DNN\", sum([p.numel() for p in model2.parameters() if p.requires_grad]))\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                torch.save(model.state_dict(), os.path.join(save_dir, f'model_epoch_{epoch}.pth'))\n",
        "\n",
        "        for threshold in threshold_values:\n",
        "            # Clone the model for each threshold\n",
        "            model_pruned = copy.deepcopy(model2)\n",
        "\n",
        "            # Apply naive pruning with current threshold\n",
        "            for module in model_pruned.modules():\n",
        "                if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "                    naive_prune(module, threshold)\n",
        "\n",
        "            # Calculate the number of nonzero parameters after pruning\n",
        "            num_nonzero = count_nonzero_params(model_pruned)\n",
        "            num_params_pruned.append(num_nonzero)\n",
        "\n",
        "            # Evaluate the model's accuracy after pruning\n",
        "            accuracy = test(args, model_pruned, device, test_loader)\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "            # Calculate percentages of parameters kept\n",
        "            params_kept_percentages = [100 * num / num_params_unpruned for num in num_params_pruned]\n",
        "\n",
        "        # Calculate and print the number of nonzero parameters and the model's accuracy\n",
        "        num_nonzero = count_nonzero_params(model2)\n",
        "        print(\"Number of nonzero parameters after pruning:\", num_nonzero)\n",
        "\n",
        "        # Assuming 'test' function returns accuracy, adjust if it returns different metrics\n",
        "        accuracy = test(args, model2, device, test_loader)\n",
        "        print(\"Model accuracy after pruning:\", accuracy)\n",
        "\n",
        "        # Calculate percentages of parameters kept\n",
        "        params_kept_percentages = [100 * num / num_params_unpruned for num in num_params_pruned]\n",
        "\n",
        "        # Plotting accuracy vs number of parameters kept, only showing accuracies >= 90%\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        for num, acc, pct in zip(num_params_pruned, accuracies, params_kept_percentages):\n",
        "            if acc >= 90:  # Only plot if accuracy is >= 90%\n",
        "                plt.scatter(num, acc, marker='o')\n",
        "                plt.annotate(f\"{pct:.1f}%\", (num, acc), textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
        "\n",
        "        plt.xlabel(\"Number of Parameters Kept\")\n",
        "        plt.ylabel(\"Test Set Accuracy\")\n",
        "        plt.title(\"Test Set Accuracy vs Number of Parameters Kept (Accuracies >= 90%)\")\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        # We now train normally, setting GoF to 0\n",
        "        for epoch in range(1, trainCycles + 1):\n",
        "            # Split every splitFreq many cycles\n",
        "            lr = .96 * lr\n",
        "            if epoch % splitFreq == 0 and epoch != 0:\n",
        "                for i, layer in enumerate(model.fc):\n",
        "                    print(layer.split(bema_scheduler(epoch), save_name=f'layer_{i}_epoch_{epoch}', show=showSpectrumDuringSplit))\n",
        "                optimizer2 = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "            y_train.append(train(args, model, device, train_loader, optimizer2, epoch, showTrainingLoss))\n",
        "            y_test.append(test(args, model, device, test_loader))\n",
        "            print(\"Number of parameters of DNN\", sum([p.numel() for p in model.parameters() if p.requires_grad]))\n",
        "\n",
        "        yTestDataStock.append(y_test)\n",
        "        yTestDataModified.append(y_test2)\n",
        "        yTrainDataStock.append(y_train)\n",
        "        yTrainDataModified.append(y_train2)\n",
        "\n",
        "    xData = [i for i in range(len(yTestDataStock[0]))]\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(xData, yTrainDataStock[0], label=\"Training Accuracy\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Training Accuracy vs Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(xData, [acc for acc in yTestDataStock[0] if acc >= 90], label=\"Test Accuracy (>= 90%)\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.title(\"Test Accuracy (>= 90%) vs Epochs\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    graphData(xData, yTestDataStock, title=\"Normal Test\")\n",
        "    graphData(xData, yTestDataModified, title=\"Pruned Test\")\n",
        "    graphData(xData, yTrainDataStock, title=\"Normal Train\")\n",
        "    graphData(xData, yTrainDataModified, title=\"Pruned Train\")\n",
        "    graphDataBoth(xData, yTestDataStock, yTestDataModified)\n",
        "    graphDataBoth(xData, yTrainDataStock, yTrainDataModified)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay6Lq09pieq8"
      },
      "outputs": [],
      "source": [
        "#training hyperparameters#\n",
        "trainCycles = 20\n",
        "seed = 4434543598112\n",
        "lr = 0.01\n",
        "m=20\n",
        "momentum = 0.9\n",
        "batchSize = 128\n",
        "splitFreq = 4\n",
        "showTrainingLoss = False\n",
        "showSpectrumDuringSplit = False\n",
        "goodnessOfFitCutoff = [1]\n",
        "\n",
        "#network topology setup\n",
        "dims = [28*28, 1000, 10]\n",
        "no_rel = [False for i in range(len(dims))] #Layers without relu\n",
        "\n",
        "\n",
        "\n",
        "def count_total_params(model):\n",
        "    return sum(p.numel() for p in model.parameters())\n",
        "\n",
        "\n",
        "def naive_prune(layer, threshold):\n",
        "    with torch.no_grad():\n",
        "        weight_mask = torch.abs(layer.weight) > threshold\n",
        "        layer.weight *= weight_mask.float()\n",
        "\n",
        "def count_nonzero_params(model):\n",
        "    count = 0\n",
        "    for param in model.parameters():\n",
        "        count += torch.count_nonzero(param).item()\n",
        "    return count\n",
        "\n",
        "\n",
        "alg = [0]\n",
        "\n",
        "#new parameters\n",
        "trialRuns = 1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  use_cuda = not False and torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda:0\" if use_cuda else \"cpu:1\")\n",
        "\n",
        "yTestDataStock = []\n",
        "yTestDataModified = []\n",
        "yTrainDataStock = []\n",
        "yTrainDataModified = []\n",
        "\n",
        "for trainCycle in range(trialRuns):\n",
        "\n",
        "  seed = seed + trainCycle  # Adjust the seed calculation\n",
        "  torch.manual_seed(seed)\n",
        "  print(\"Seed:\", seed)  # Print the seed value\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  x, y_test, y_test2, y_train, y_train2 = [], [], [], [], []\n",
        "\n",
        "  torch.manual_seed(seed)  # Ensure `model2` gets the same initial weights as `model`\n",
        "  print(\"Seed:\", seed)  # Print the seed value\n",
        "\n",
        "  model = networkModelModified(no_rel, dims, alpha=0.25, beta=0.9, goodnessOfFitCutoff=[0])\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  (args, model,optimizer, test_loader, train_loader) = neuralInit(seed, device, model, lr, momentum, batchSize)\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "  model2 = networkModelModified(no_rel, dims, alpha=0.25, beta=0.1, goodnessOfFitCutoff=goodnessOfFitCutoff)\n",
        "\n",
        "  model2.to(device)\n",
        "\n",
        "  (args, model2,optimizer, test_loader, train_loader) = neuralInit(seed, device, model2, lr, momentum, batchSize)\n",
        "\n",
        "  optimizer2 = optim.SGD(model2.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "  num_params_unpruned = sum([p.numel() for p in model2.parameters() if p.requires_grad])\n",
        "  accuracies = []\n",
        "  num_params_pruned = []\n",
        "  threshold_values = [1/(1+k) for k in range(m)]  # Example thresholds\n",
        "\n",
        "  l = 5  # Define the frequency of pruning, every 'l' epochs\n",
        "  initial_threshold = 0.001\n",
        "  max_threshold = 0.02  # Maximum threshold value\n",
        "  threshold_increment = (max_threshold - initial_threshold) / trainCycles  # Increment per epoch\n",
        "\n",
        "  # Function to calculate the pruning threshold based on the current epoch\n",
        "  def calculate_pruning_threshold(epoch, initial_threshold=0.01, increment=0.01):\n",
        "      return min(initial_threshold + increment * epoch, max_threshold)\n",
        "\n",
        "  # Training loop\n",
        "  for epoch in range(1, trainCycles + 1):\n",
        "      # Update learning rate and split at specified frequency\n",
        "      lr *= 0.96\n",
        "      if epoch % splitFreq == 0 and epoch != 0:\n",
        "          for i, layer in enumerate(model2.fc):\n",
        "              print(layer.split(bema_scheduler(epoch), save_name=f'layer_{i}_epoch_{epoch}', show=showSpectrumDuringSplit))\n",
        "              print(\"Cutting. At this epoch we keep\", bema_scheduler(epoch))\n",
        "          optimizer2 = optim.SGD(model2.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "      # Training and testing\n",
        "      y_train2.append(train(args, model2, device, train_loader, optimizer2, epoch, showTrainingLoss))\n",
        "      y_test2.append(test(args, model2, device, test_loader))\n",
        "\n",
        "      # Prune every 'l' epochs\n",
        "      if epoch % l == 0:\n",
        "          threshold = calculate_pruning_threshold(epoch, initial_threshold, threshold_increment)\n",
        "          for module in model2.modules():\n",
        "              if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "                  naive_prune(module, threshold)\n",
        "\n",
        "          # Calculate and print the number of nonzero parameters after pruning\n",
        "          num_nonzero = count_nonzero_params(model2)\n",
        "          print(f\"Epoch {epoch}: Pruned with threshold {threshold}. Nonzero parameters: {num_nonzero}\")\n",
        "\n",
        "          # Evaluate the model's accuracy after pruning\n",
        "          accuracy = test(args, model2, device, test_loader)\n",
        "          print(f\"Model accuracy after pruning at Epoch {epoch}: {accuracy}\")\n",
        "\n",
        "      print(\"Number of parameters of DNN\", sum([p.numel() for p in model2.parameters() if p.requires_grad]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udvW-Ebp8GnA"
      },
      "outputs": [],
      "source": [
        "#training hyperparameters#\n",
        "trainCycles = 20\n",
        "seed = 4434543598112\n",
        "lr = 0.02\n",
        "momentum = 0.9\n",
        "batchSize = 128\n",
        "m=50\n",
        "splitFreq = 5\n",
        "showTrainingLoss = False\n",
        "showSpectrumDuringSplit = False\n",
        "goodnessOfFitCutoff = [1]\n",
        "\n",
        "#network topology setup\n",
        "dims = [28*28, 1000,1000, 10]\n",
        "no_rel = [False for i in range(len(dims))] #Layers without relu\n",
        "\n",
        "\n",
        "\n",
        "alg = [0]\n",
        "\n",
        "#new parameters\n",
        "trialRuns = 1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  use_cuda = not False and torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda:0\" if use_cuda else \"cpu:1\")\n",
        "\n",
        "yTestDataStock = []\n",
        "yTestDataModified = []\n",
        "yTrainDataStock = []\n",
        "yTrainDataModified = []\n",
        "\n",
        "for trainCycle in range(trialRuns):\n",
        "\n",
        "  seed = seed + trainCycle  # Adjust the seed calculation\n",
        "  torch.manual_seed(seed)\n",
        "  print(\"Seed:\", seed)  # Print the seed value\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  x, y_test, y_test2, y_train, y_train2 = [], [], [], [], []\n",
        "\n",
        "  torch.manual_seed(seed)  # Ensure `model2` gets the same initial weights as `model`\n",
        "  print(\"Seed:\", seed)  # Print the seed value\n",
        "\n",
        "  model = networkModelModified(no_rel, dims, alpha=0.25, beta=0.1, goodnessOfFitCutoff=[0])\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  (args, model,optimizer, test_loader, train_loader) = neuralInit(seed, device, model, lr, momentum, batchSize)\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "  model2 = networkModelModified(no_rel, dims, alpha=0.25, beta=0.9, goodnessOfFitCutoff=goodnessOfFitCutoff)\n",
        "\n",
        "  model2.to(device)\n",
        "\n",
        "  (args, model2,optimizer, test_loader, train_loader) = neuralInit(seed, device, model2, lr, momentum, batchSize)\n",
        "\n",
        "  optimizer2 = optim.SGD(model2.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "\n",
        "\n",
        "  num_params_unpruned = sum([p.numel() for p in model2.parameters() if p.requires_grad])\n",
        "  accuracies = []\n",
        "  num_params_pruned = []\n",
        "  threshold_values = [1/(1+k) for k in range(m)]  # Example thresholds\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for epoch in range(1,trainCycles):\n",
        "    #split every splitFreq many cycles\n",
        "    if epoch % splitFreq == 0 and epoch != 0:\n",
        "      lr=.96*lr\n",
        "      for i,layer in enumerate(model2.fc):\n",
        "        print(layer.split(bema_scheduler(epoch),save_name=f'layer_{i}_epoch_{epoch}',show=showSpectrumDuringSplit))\n",
        "        print(\"cutting. at this epoch we keep\", bema_scheduler(epoch))\n",
        "      optimizer2 = optim.SGD(model2.parameters(), lr, momentum=args['momentum'])\n",
        "\n",
        "    y_train2.append(train(args, model2, device, train_loader, optimizer2, epoch, showTrainingLoss))\n",
        "    y_test2.append(test(args, model2, device, test_loader))\n",
        "    print(\"Numer of parameters of DNN\", sum([p.numel() for p in model2.parameters() if p.requires_grad]))\n",
        "\n",
        "  for threshold in threshold_values:\n",
        "    # Clone the model for each threshold\n",
        "    model_pruned = copy.deepcopy(model2)\n",
        "\n",
        "    # Apply naive pruning with current threshold\n",
        "    for module in model_pruned.modules():\n",
        "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "            naive_prune(module, threshold)\n",
        "\n",
        "    # Calculate the number of nonzero parameters after pruning\n",
        "    num_nonzero = count_nonzero_params(model_pruned)\n",
        "    num_params_pruned.append(num_nonzero)\n",
        "\n",
        "    # Evaluate the model's accuracy after pruning\n",
        "    accuracy = test(args, model_pruned, device, test_loader)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    # Calculate percentages of parameters kept\n",
        "    params_kept_percentages = [100 * num / num_params_unpruned for num in num_params_pruned]\n",
        "\n",
        "  # Calculate and print the number of nonzero parameters and the model's accuracy\n",
        "  num_nonzero = count_nonzero_params(model2)\n",
        "  print(\"Number of nonzero parameters after pruning:\", num_nonzero)\n",
        "\n",
        "  accuracy = test(args, model2, device, test_loader)\n",
        "  print(\"Model accuracy after pruning:\", accuracy)\n",
        "\n",
        "  # Calculate percentages of parameters kept\n",
        "  params_kept_percentages = [100 * num / num_params_unpruned for num in num_params_pruned]\n",
        "\n",
        "  # Plotting accuracy vs number of parameters kept, only showing accuracies >= 88%\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  for num, acc, pct in zip(num_params_pruned, accuracies, params_kept_percentages):\n",
        "      if acc >= 85:  # Only plot if accuracy is >= 85%\n",
        "          plt.scatter(num, acc, marker='o')\n",
        "          plt.annotate(f\"{pct:.1f}%\", (num, acc), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "  plt.xlabel(\"Number of Parameters Kept\")\n",
        "  plt.ylabel(\"Test Set Accuracy\")\n",
        "  plt.title(\"Test Set Accuracy vs Number of Parameters Kept (Accuracies >= 85%)\")\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6nsRBfRYhyK"
      },
      "outputs": [],
      "source": [
        "#training hyperparameters#\n",
        "trainCycles = 20\n",
        "seed = 4434543598112\n",
        "lr = 0.02\n",
        "momentum = 0.9\n",
        "batchSize = 128\n",
        "m=50\n",
        "splitFreq = 5\n",
        "showTrainingLoss = False\n",
        "showSpectrumDuringSplit = False\n",
        "goodnessOfFitCutoff = [0]\n",
        "\n",
        "#network topology setup\n",
        "dims = [28*28, 1000,1000, 10]\n",
        "no_rel = [False for i in range(len(dims))] #Layers without relu\n",
        "\n",
        "\n",
        "\n",
        "alg = [0]\n",
        "\n",
        "#new parameters\n",
        "trialRuns = 1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  use_cuda = not False and torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda:0\" if use_cuda else \"cpu:1\")\n",
        "\n",
        "yTestDataStock = []\n",
        "yTestDataModified = []\n",
        "yTrainDataStock = []\n",
        "yTrainDataModified = []\n",
        "\n",
        "for trainCycle in range(trialRuns):\n",
        "\n",
        "  seed = seed + trainCycle  # Adjust the seed calculation\n",
        "  torch.manual_seed(seed)\n",
        "  print(\"Seed:\", seed)  # Print the seed value\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  x, y_test, y_test2, y_train, y_train2 = [], [], [], [], []\n",
        "\n",
        "  torch.manual_seed(seed)  # Ensure `model2` gets the same initial weights as `model`\n",
        "  print(\"Seed:\", seed)  # Print the seed value\n",
        "\n",
        "  model = networkModelModified(no_rel, dims, alpha=0.25, beta=0.1, goodnessOfFitCutoff=[0])\n",
        "\n",
        "  model.to(device)\n",
        "\n",
        "  (args, model,optimizer, test_loader, train_loader) = neuralInit(seed, device, model, lr, momentum, batchSize)\n",
        "\n",
        "  optimizer = optim.SGD(model.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "  model2 = networkModelModified(no_rel, dims, alpha=0.25, beta=0.9, goodnessOfFitCutoff=goodnessOfFitCutoff)\n",
        "\n",
        "  model2.to(device)\n",
        "\n",
        "  (args, model2,optimizer, test_loader, train_loader) = neuralInit(seed, device, model2, lr, momentum, batchSize)\n",
        "\n",
        "  optimizer2 = optim.SGD(model2.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "\n",
        "\n",
        "  num_params_unpruned = sum([p.numel() for p in model2.parameters() if p.requires_grad])\n",
        "  accuracies = []\n",
        "  num_params_pruned = []\n",
        "  threshold_values = [1/(1+k) for k in range(m)]  # Example thresholds\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  for epoch in range(1,trainCycles):\n",
        "    #split every splitFreq many cycles\n",
        "    if epoch % splitFreq == 0 and epoch != 0:\n",
        "      lr=.96*lr\n",
        "      for i,layer in enumerate(model2.fc):\n",
        "        print(layer.split(bema_scheduler(epoch),save_name=f'layer_{i}_epoch_{epoch}',show=showSpectrumDuringSplit))\n",
        "        print(\"cutting. at this epoch we keep\", bema_scheduler(epoch))\n",
        "      optimizer2 = optim.SGD(model2.parameters(), lr, momentum=args['momentum'])\n",
        "\n",
        "    y_train2.append(train(args, model2, device, train_loader, optimizer2, epoch, showTrainingLoss))\n",
        "    y_test2.append(test(args, model2, device, test_loader))\n",
        "    print(\"Numer of parameters of DNN\", sum([p.numel() for p in model2.parameters() if p.requires_grad]))\n",
        "\n",
        "  for threshold in threshold_values:\n",
        "    # Clone the model for each threshold\n",
        "    model_pruned = copy.deepcopy(model2)\n",
        "\n",
        "    # Apply naive pruning with current threshold\n",
        "    for module in model_pruned.modules():\n",
        "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "            naive_prune(module, threshold)\n",
        "\n",
        "    # Calculate the number of nonzero parameters after pruning\n",
        "    num_nonzero = count_nonzero_params(model_pruned)\n",
        "    num_params_pruned.append(num_nonzero)\n",
        "\n",
        "    # Evaluate the model's accuracy after pruning\n",
        "    accuracy = test(args, model_pruned, device, test_loader)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    # Calculate percentages of parameters kept\n",
        "    params_kept_percentages = [100 * num / num_params_unpruned for num in num_params_pruned]\n",
        "\n",
        "  # Calculate and print the number of nonzero parameters and the model's accuracy\n",
        "  num_nonzero = count_nonzero_params(model2)\n",
        "  print(\"Number of nonzero parameters after pruning:\", num_nonzero)\n",
        "\n",
        "  accuracy = test(args, model2, device, test_loader)\n",
        "  print(\"Model accuracy after pruning:\", accuracy)\n",
        "\n",
        "  # Calculate percentages of parameters kept\n",
        "  params_kept_percentages = [100 * num / num_params_unpruned for num in num_params_pruned]\n",
        "\n",
        "  # Plotting accuracy vs number of parameters kept, only showing accuracies >= 70%\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  for num, acc, pct in zip(num_params_pruned, accuracies, params_kept_percentages):\n",
        "      if acc >= 70:  # Only plot if accuracy is >= 70%\n",
        "          plt.scatter(num, acc, marker='o')\n",
        "          plt.annotate(f\"{pct:.1f}%\", (num, acc), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "  plt.xlabel(\"Number of Parameters Kept\")\n",
        "  plt.ylabel(\"Test Set Accuracy\")\n",
        "  plt.title(\"Test Set Accuracy vs Number of Parameters Kept (Accuracies >= 85%)\")\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RnbVkp1uG3Wf"
      },
      "outputs": [],
      "source": [
        "def graphDataBothTest(xData, yData0, yData1):\n",
        "  L = [yData0, yData1]\n",
        "  Labels = [\"Normal\", \"Pruned\"]\n",
        "  colors = [\"black\", \"red\"]\n",
        "  plt.title(\"Accuracy on Test Set of Normal Vs Pruned DNNs\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  for i in range(2):\n",
        "    mean, variance = getMeanAndVar(L[i])\n",
        "    std = np.sqrt(variance)\n",
        "    plt.plot(xData, mean, label = \"Mean \" + Labels[i], color = colors[i])\n",
        "    plt.plot(xData, mean - std, \"--\", label = \"Mean - 1std \" +  Labels[i], color = colors[i])\n",
        "    plt.plot(xData, mean + std, \"--\", label = \"Mean + 1std \" + Labels[i], color = colors[i])\n",
        "    plt.fill_between(xData, mean, mean + std, color = \"grey\", alpha = 0.5)\n",
        "    plt.fill_between(xData, mean, mean - std, color = \"grey\", alpha = 0.5)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def graphDataBothTrain(xData, yData0, yData1):\n",
        "  L = [yData0, yData1]\n",
        "  Labels = [\"Normal\", \"Pruned\"]\n",
        "  colors = [\"black\", \"red\"]\n",
        "  plt.title(\"Accuracy on Training Set of Normal Vs Pruned DNNs\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  for i in range(2):\n",
        "    mean, variance = getMeanAndVar(L[i])\n",
        "    std = np.sqrt(variance)\n",
        "    plt.plot(xData, mean, label = \"Mean \" + Labels[i], color = colors[i])\n",
        "    plt.plot(xData, mean - std, \"--\", label = \"Mean - 1std \" +  Labels[i], color = colors[i])\n",
        "    plt.plot(xData, mean + std, \"--\", label = \"Mean + 1std \" + Labels[i], color = colors[i])\n",
        "    plt.fill_between(xData, mean, mean + std, color = \"grey\", alpha = 0.5)\n",
        "    plt.fill_between(xData, mean, mean - std, color = \"grey\", alpha = 0.5)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "xData = [i for i in range(len(yTestDataStock[0]))]\n",
        "graphData(xData, yTestDataStock, title = \"Normal Test\")\n",
        "graphData(xData, yTestDataModified, title = \"Pruned Test\")\n",
        "graphData(xData, yTrainDataStock, title = \"Normal Train\")\n",
        "graphData(xData, yTrainDataModified, title = \"Pruned Train\")\n",
        "graphDataBothTest(xData, yTestDataStock, yTestDataModified)\n",
        "graphDataBothTrain(xData, yTrainDataStock, yTrainDataModified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWifa3F2n1QC"
      },
      "outputs": [],
      "source": [
        "#training hyperparameters#\n",
        "trainCycles = 30\n",
        "seed = 44345434\n",
        "lr = 0.02\n",
        "lr1 = lr\n",
        "momentum = .9\n",
        "batchSize = 128\n",
        "splitFreq = 11\n",
        "m=50\n",
        "showTrainingLoss = False\n",
        "showSpectrumDuringSplit = False\n",
        "goodnessOfFitCutoff = [.6, .05]\n",
        "\n",
        "#network topology setup\n",
        "conv_dims = [1,32, 64, 128, 256, 512] # article [1, 64, 128, 256, 512]\n",
        "original_conv_dims =  conv_dims\n",
        "\n",
        "fc_dims = [4608, 1000,   10]\n",
        "original_fc_dims = fc_dims\n",
        "\n",
        "\n",
        "#dims = [conv_dims, fc_dims]\n",
        "#original_dims = dims\n",
        "\n",
        "c=[False for i in range(len(fc_dims) - 1)]\n",
        "c.insert(0,True)\n",
        "no_rel = [[True for i in range(len(conv_dims))],\n",
        "          c] #Layers with relu\n",
        "\n",
        "alg = [4,0]\n",
        "\n",
        "#new parameters\n",
        "trialRuns = 1\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  use_cuda = torch.cuda.is_available()\n",
        "  device = torch.device(\"cuda:0\" if use_cuda else \"cpu:1\")\n",
        "\n",
        "yTestDataStock = []\n",
        "yTestDataModified = []\n",
        "yTrainDataStock = []\n",
        "yTrainDataModified = []\n",
        "\n",
        "\n",
        "\n",
        "for trainCycle in range(trialRuns):\n",
        "  seed = seed + trainCycle  # Adjust the seed calculation\n",
        "  torch.manual_seed(seed)\n",
        "  print(\"Seed:\", seed)\n",
        "\n",
        "  model =  CNNModelModified(conv_dims, fc_dims, no_rel,alpha=0.25,beta=0.1,goodnessOfFitCutoff=[0,0])\n",
        "  model.to(device)\n",
        "\n",
        "  (args, model,optimizer, test_loader, train_loader) = neuralInit(seed, device, model, lr, momentum, batchSize)\n",
        "\n",
        "  x, y_test, y_test2, y_train, y_train2 = [], [], [], [], []\n",
        "  y_test_loss, y_test2_loss, y_train_loss, y_train2_loss = [], [], [], [],\n",
        "\n",
        "  model2 = CNNModelModified(conv_dims, fc_dims, no_rel,alpha=0.2,beta=0.9,goodnessOfFitCutoff=goodnessOfFitCutoff)\n",
        "  model2.to(device)\n",
        "  optimizer2 = optim.SGD(model2.parameters(), lr=args['lr'], momentum=args['momentum'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  num_params_unpruned = sum([p.numel() for p in model2.parameters() if p.requires_grad])\n",
        "  accuracies = []\n",
        "  num_params_pruned = []\n",
        "  threshold_values = [1/(20*(1+k)) for k in range(m)]  # Example thresholds\n",
        "\n",
        "\n",
        "\n",
        "  torch.manual_seed(trainCycle)\n",
        "  for epoch in range(1,trainCycles):\n",
        "    #split every splitFreq many cycles\n",
        "    if epoch % splitFreq == 0 and epoch != 0:\n",
        "      lr = 0.96*lr\n",
        "      for i,layer in enumerate(model2.fc):\n",
        "        print(layer.split(bema_scheduler(epoch),save_name=f'layer_{i}_epoch_{epoch}',show=showSpectrumDuringSplit, layer_type = 'fc'))\n",
        "        print(\"cutting. at this epoch we keep\", bema_scheduler(epoch))\n",
        "      for i,layer in enumerate(model2.conv):\n",
        "        print(layer.split(bema_scheduler(epoch),save_name=f'layer_{i}_epoch_{epoch}',show=showSpectrumDuringSplit, layer_type = 'conv'))\n",
        "        print(\"cutting. at this epoch we keep\", bema_scheduler(epoch))\n",
        "      optimizer2 = optim.SGD(model2.parameters(), lr = args['lr'], momentum=args['momentum'])\n",
        "\n",
        "    train_accuracy = train(args, model2, device, train_loader, optimizer2, epoch, showTrainingLoss)\n",
        "    y_train2.append(train_accuracy)\n",
        "    print(\"Numer of parameters of DNN\", sum([p.numel() for p in model2.parameters() if p.requires_grad]))\n",
        "\n",
        "  for threshold in threshold_values:\n",
        "    # Clone the model for each threshold\n",
        "    model_pruned = copy.deepcopy(model2)\n",
        "\n",
        "    # Apply naive pruning with current threshold\n",
        "    for module in model_pruned.modules():\n",
        "        if isinstance(module, torch.nn.Conv2d) or isinstance(module, torch.nn.Linear):\n",
        "            naive_prune(module, threshold)\n",
        "\n",
        "    # Calculate the number of nonzero parameters after pruning\n",
        "    num_nonzero = count_nonzero_params(model_pruned)\n",
        "    num_params_pruned.append(num_nonzero)\n",
        "\n",
        "    # Evaluate the model's accuracy after pruning\n",
        "    accuracy = test(args, model_pruned, device, test_loader)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "    # Calculate percentages of parameters kept\n",
        "    params_kept_percentages = [100 * num / num_params_unpruned for num in num_params_pruned]\n",
        "\n",
        "  # Calculate and print the number of nonzero parameters and the model's accuracy\n",
        "  num_nonzero = count_nonzero_params(model2)\n",
        "  #print(\"Number of nonzero parameters after pruning:\", num_nonzero)\n",
        "\n",
        "  # Assuming 'test' function returns accuracy, adjust if it returns different metrics\n",
        "  accuracy = test(args, model2, device, test_loader)\n",
        "  #print(\"Model accuracy after pruning:\", accuracy)\n",
        "\n",
        "  # Calculate percentages of parameters kept\n",
        "  params_kept_percentages = [100 * num / num_params_unpruned for num in num_params_pruned]\n",
        "\n",
        "  # Plotting accuracy vs number of parameters kept, only showing accuracies >= 80%\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  for num, acc, pct in zip(num_params_pruned, accuracies, params_kept_percentages):\n",
        "      if acc >= 80:  # Only plot if accuracy is >= 80%\n",
        "          plt.scatter(num, acc, marker='o')\n",
        "          plt.annotate(f\"{pct:.1f}%\", (num, acc), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "  plt.xlabel(\"Number of Parameters Kept\")\n",
        "  plt.ylabel(\"Test Set Accuracy\")\n",
        "  plt.title(\"Test Set Accuracy vs Number of Parameters Kept (Accuracies >= 80%)\")\n",
        "  plt.grid(True)\n",
        "  plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6z9P0NhnnyB"
      },
      "outputs": [],
      "source": [
        "def graphDataBothTest(xData, yData0, yData1):\n",
        "  L = [yData0, yData1]\n",
        "  Labels = [\"Normal\", \"Modified\"]\n",
        "  colors = [\"black\", \"red\"]\n",
        "  plt.title(\"Accuracy on Test Set of Normal Vs Pruned DNNs\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  for i in range(2):\n",
        "    mean, variance = getMeanAndVar(L[i])\n",
        "    std = np.sqrt(variance)\n",
        "    plt.plot(xData, mean, label = \"Mean \" + Labels[i], color = colors[i])\n",
        "    plt.plot(xData, mean - std, \"--\", label = \"Mean - 1std \" +  Labels[i], color = colors[i])\n",
        "    plt.plot(xData, mean + std, \"--\", label = \"Mean + 1std \" + Labels[i], color = colors[i])\n",
        "    plt.fill_between(xData, mean, mean + std, color = \"grey\", alpha = 0.5)\n",
        "    plt.fill_between(xData, mean, mean - std, color = \"grey\", alpha = 0.5)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def graphDataBothTrain(xData, yData0, yData1):\n",
        "  L = [yData0, yData1]\n",
        "  Labels = [\"Normal\", \"Pruned\"]\n",
        "  colors = [\"black\", \"red\"]\n",
        "  plt.title(\"Accuracy on Training Set of Normal Vs Pruned DNNs\")\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Accuracy\")\n",
        "  for i in range(2):\n",
        "    mean, variance = getMeanAndVar(L[i])\n",
        "    std = np.sqrt(variance)\n",
        "    plt.plot(xData, mean, label = \"Mean \" + Labels[i], color = colors[i])\n",
        "    plt.plot(xData, mean - std, \"--\", label = \"Mean - 1std \" +  Labels[i], color = colors[i])\n",
        "    plt.plot(xData, mean + std, \"--\", label = \"Mean + 1std \" + Labels[i], color = colors[i])\n",
        "    plt.fill_between(xData, mean, mean + std, color = \"grey\", alpha = 0.5)\n",
        "    plt.fill_between(xData, mean, mean - std, color = \"grey\", alpha = 0.5)\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "xData = [i for i in range(len(yTestDataStock[0]))]\n",
        "graphData(xData, yTestDataStock, title = \"Normal Test\")\n",
        "graphData(xData, yTestDataModified, title = \"Pruned Test\")\n",
        "graphData(xData, yTrainDataStock, title = \"Normal Train\")\n",
        "graphData(xData, yTrainDataModified, title = \"Pruned Train\")\n",
        "graphDataBothTest(xData, yTestDataStock, yTestDataModified)\n",
        "graphDataBothTrain(xData, yTrainDataStock, yTrainDataModified)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}